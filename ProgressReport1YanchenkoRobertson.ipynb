{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress Report 1 - Anna Yanchenko, Megan Robertson**\n",
    "\n",
    "*Paper*\n",
    "\n",
    "David M. Blei, Andrew Y. Ng, Michael I. Jordan - Latent Dirichlet Allocatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*github*\n",
    "\n",
    "https://github.com/mrobertson15/663FinalProject/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Headers*\n",
    "\n",
    "Background - LDA is often used in text mining because it is a generative probabilistic hierarchical Bayesian model that is used for groups of discrete data.\n",
    "\n",
    "Implementation - Will be implemented using Python\n",
    "\n",
    "Testing\n",
    "\n",
    "Optimization - Using numpy to vecorize where possible, also potentially using methods besides the variational inference explained in the paper (i.e. MCMC)\n",
    "\n",
    "Application and Comparison - Using data from the Gutenburg Project, comparing to methods such as tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Abstract*\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a modeling process often used to analyze collections of texts. This Bayesian hierarchical model has three levels. Each of the documents in the collection is modeled as a mixture over latent topics, and each topic is modeled as a mixture over a set of topic probabilities. In other words, each topic has a distribution over words. LDA has been shown to be an improvement over other methods with similar inference goals such as the term-frequency inverse-document frequency (tf-idf) scheme and probabilistic latent semantic indexing (pLSI) scheme. The goal of this project is to implement LDA using variational inference as well as MCMC to estimate the model parameters. In order to test the performance of the algorithm, we will perform document classification on a collection of texts from the Gutenberg Corpus. (www.gutenberg.org)  We will also classify the documents using one of the competing algorithms, such as tf-idf or pLSI, and compare the resulting classifications from the various algorithms.  Optimization will be attempted using vectorization and additional methods to be learned in the coming weeks. Parallelization will not be possible given that each iteration of the algorithm depends on the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
