{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in c:\\users\\megan robertson\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.0.3, however version 8.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 The quick brown fox\n",
      "s2 Brown fox jumps over the jumps jumps jumps\n",
      "s3 The the the lazy dog elephant.\n",
      "s4 The the the the the dog peacock lion tiger elephant\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4 \n",
    "for k, v in docs.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "\n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    print(\"here\")\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "            #Remove stop words#\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "            \n",
    "            #remove stop words\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "        \n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = wordOrder.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'elephant',\n",
       "  'fox',\n",
       "  'jumps',\n",
       "  'lazy',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs)\n",
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  The output of this function are the matricies gamma and phi, where gamma (k vector) is the Dirichlet paramteters and the matrix phi (N x k, where k is the number of topics) are the multinomial paramters.  See page 1004 of paper for derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    newPhi = oldPhi\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,k))\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*np.exp(scipy.special.psi(gamma[i]))\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        for i in range(0,k):\n",
    "            gamma[i] = alpha[i] + np.sum(newPhi[:, i]) #updating gamma\n",
    "\n",
    "\n",
    "        criteria = (1/(N*k)*np.sum((newPhi - oldPhi)**2))**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 for derivation.  \n",
    "\n",
    "The alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the Mstep() function maximizes for alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "def alphaUpdate(k, M, alphaOld, gamma, tol):\n",
    "    h = np.zeros(k)\n",
    "    g = np.zeros(k)\n",
    "    alphaNew = np.zeros(k)\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(0, M):\n",
    "                docSum += scipy.special.psi(gamma[d][i]) - scipy.special.psi(np.sum(gamma[d]))\n",
    "            g[i] = M*(scipy.special.psi(sum(alphaOld)) - scipy.special.psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*scipy.special.polygamma(1, alphaOld[i])\n",
    "        z =  -scipy.special.polygamma(1, np.sum(alphaOld))\n",
    "        c = np.sum(g/h)/(1/z + np.sum(1/h))\n",
    "        step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            alphaNew = alphaOld + step\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol):\n",
    "    #Calculate beta#\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(0,M):\n",
    "                Nd = corpusMatrix[0][d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi[d][n,i]*corpusMatrix[0][d][n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    ##Update ALPHA##\n",
    "    alphaNew = alphaUpdate(k, M, alphaOld, gamma, tol)\n",
    "    return(alphaNew, beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Function:**\n",
    "Finally, we implement the entire Latent Dirichlet Allocation method in the LDA function, which takes as its arguments k (the number of topics), D (the number of documents in the corpus), a corpus matrix (the output from make_word_matrix above) and a tolerance (which sets the convergence criteria for the while loops).  For each document d, the function runs until the alpha or beta parameters converge, by first running the E step and then the M step for each document separately.  The final values of phi, gamma, alpha and beta are returned for all D documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, M, corpusMatrix, tol):\n",
    "\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = 10*np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        alphaNew, betaNew = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[array([[ 0.12289737,  0.15669001,  0.72041262],\n",
       "          [ 0.43619702,  0.19079607,  0.37300691],\n",
       "          [ 0.00195915,  0.08277618,  0.91526466]]),\n",
       "   array([[ 0.47978723,  0.21055167,  0.3096611 ],\n",
       "          [ 0.00252532,  0.10704752,  0.89042716],\n",
       "          [ 0.56793174,  0.2219656 ,  0.21010266],\n",
       "          [ 0.56793174,  0.2219656 ,  0.21010266],\n",
       "          [ 0.56793174,  0.2219656 ,  0.21010266],\n",
       "          [ 0.56793174,  0.2219656 ,  0.21010266]]),\n",
       "   array([[ 0.51061311,  0.15562109,  0.3337658 ],\n",
       "          [ 0.1980239 ,  0.18810243,  0.61387368],\n",
       "          [ 0.44255284,  0.078629  ,  0.47881815]]),\n",
       "   array([[ 0.1909399 ,  0.17839313,  0.63066696],\n",
       "          [ 0.25619717,  0.09972446,  0.64407837],\n",
       "          [ 0.13248979,  0.0094582 ,  0.858052  ],\n",
       "          [ 0.55265073,  0.10621655,  0.34113273],\n",
       "          [ 0.42963913,  0.07508032,  0.49528055]])],\n",
       "  [array([  6.23114479,   2.97648098,  10.73108617]),\n",
       "   array([  8.42413077,   3.75168029,  10.76290088]),\n",
       "   array([  6.8212811 ,   2.96857124,  10.1488596 ]),\n",
       "   array([  7.23200797,   3.01509139,  11.69161259])],\n",
       "  array([ 6.03602026,  2.71593489,  9.11778587]),\n",
       "  array([[ 0.15194989,  0.06452404,  0.14468532,  0.00074392,  0.37684999,\n",
       "           0.08470408,  0.02197834,  0.04249978,  0.02038708,  0.09167758],\n",
       "         [ 0.158827  ,  0.14503481,  0.06082803,  0.07511972,  0.35135746,\n",
       "           0.06158458,  0.00374293,  0.03946438,  0.06200759,  0.04203351],\n",
       "         [ 0.08083837,  0.14737271,  0.11534824,  0.21382162,  0.09951752,\n",
       "           0.03952299,  0.10160653,  0.07626876,  0.08530791,  0.04039535]])]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA(3, 4, corpusMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps\n",
    "\n",
    "- Compare output to Python package\n",
    "- Test on corpus in paper\n",
    "- Model topics on different corpus\n",
    "- Time and optimize (use Cython, quite slow now)\n",
    "- Run collaborative filtering\n",
    "- Compare to Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/annayanchenko/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Union Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to C:\\Users\\Megan\n",
      "[nltk_data]     Robertson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\state_union.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"state_union\")\n",
    "from ntlk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2005-GWBush', '2001-GWBush-1', '1977-Ford', '1985-Reagan', '1979-Carter', '1947-Truman', '1965-Johnson-2', '1970-Nixon', '1981-Reagan', '1969-Johnson', '1960-Eisenhower', '1986-Reagan', '1959-Eisenhower', '1968-Johnson', '1976-Ford', '1966-Johnson', '1980-Carter', '1961-Kennedy', '1958-Eisenhower', '1973-Nixon', '1974-Nixon', '1999-Clinton', '1949-Truman', '2001-GWBush-2', '2003-GWBush', '2004-GWBush', '1972-Nixon', '1954-Eisenhower', '1965-Johnson-1', '1993-Clinton', '1964-Johnson', '1953-Eisenhower', '1994-Clinton', '2006-GWBush', '1988-Reagan', '1962-Kennedy', '1989-Bush', '1997-Clinton', '1956-Eisenhower', '1996-Clinton', '1948-Truman', '1982-Reagan', '2002-GWBush', '1957-Eisenhower', '1950-Truman', '1971-Nixon', '1992-Bush', '1990-Bush', '1991-Bush-2', '1991-Bush-1', '1963-Kennedy', '1983-Reagan', '1955-Eisenhower', '1951-Truman', '1967-Johnson', '1995-Clinton', '1987-Reagan', '1978-Carter', '1946-Truman', '1963-Johnson', '2000-Clinton', '1998-Clinton', '1945-Truman', '1984-Reagan', '1975-Ford'])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dictionary of the state of the Union Addresses\n",
    "SOTU = {}\n",
    "fileNames = state_union.fileids()\n",
    "for i in range(0, len(fileNames)):\n",
    "    SOTU[fileNames[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(fileNames[i]))\n",
    "    \n",
    "#contractions are split\n",
    "SOTU.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing the new matrix function on one SOTU\n",
    "test = {}\n",
    "test[\"2005-GWBush\"] = list(nltk.corpus.state_union.words(\"2005-GWBush.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]])],\n",
       " ['--',\n",
       "  '.\"',\n",
       "  '.\\'\"',\n",
       "  '.)',\n",
       "  '000',\n",
       "  '03',\n",
       "  '10',\n",
       "  '11th',\n",
       "  '13',\n",
       "  '150',\n",
       "  '1990s',\n",
       "  '2',\n",
       "  '200',\n",
       "  '2001',\n",
       "  '2005',\n",
       "  '2009',\n",
       "  '2018',\n",
       "  '2027',\n",
       "  '2033',\n",
       "  '2042',\n",
       "  '20s',\n",
       "  '20th',\n",
       "  '21st',\n",
       "  '275',\n",
       "  '28',\n",
       "  '3',\n",
       "  '300',\n",
       "  '35',\n",
       "  '350',\n",
       "  '45',\n",
       "  '55',\n",
       "  '60',\n",
       "  '9',\n",
       "  'abandon',\n",
       "  'abbas',\n",
       "  'able',\n",
       "  'abolition',\n",
       "  'abroad',\n",
       "  'access',\n",
       "  'accomplished',\n",
       "  'account',\n",
       "  'accountability',\n",
       "  'accounts',\n",
       "  'accumulates',\n",
       "  'achieve',\n",
       "  'achieved',\n",
       "  'achievement',\n",
       "  'achievements',\n",
       "  'across',\n",
       "  'act',\n",
       "  'action',\n",
       "  'actions',\n",
       "  'active',\n",
       "  'activist',\n",
       "  'adapting',\n",
       "  'add',\n",
       "  'added',\n",
       "  'addition',\n",
       "  'additional',\n",
       "  'address',\n",
       "  'administration',\n",
       "  'adulthood',\n",
       "  'advance',\n",
       "  'advancement',\n",
       "  'advances',\n",
       "  'advantage',\n",
       "  'advocates',\n",
       "  'afford',\n",
       "  'affordable',\n",
       "  'afghanistan',\n",
       "  'afloat',\n",
       "  'afraid',\n",
       "  'african',\n",
       "  'afterward',\n",
       "  'age',\n",
       "  'agencies',\n",
       "  'agenda',\n",
       "  'aggressive',\n",
       "  'ago',\n",
       "  'agree',\n",
       "  'ahead',\n",
       "  'aids',\n",
       "  'aim',\n",
       "  'air',\n",
       "  'al',\n",
       "  'allawi',\n",
       "  'allies',\n",
       "  'allows',\n",
       "  'ally',\n",
       "  'alone',\n",
       "  'along',\n",
       "  'already',\n",
       "  'also',\n",
       "  'alternative',\n",
       "  'always',\n",
       "  'ambitions',\n",
       "  'ambitious',\n",
       "  'amendment',\n",
       "  'america',\n",
       "  'american',\n",
       "  'americans',\n",
       "  'amnesty',\n",
       "  'among',\n",
       "  'annual',\n",
       "  'another',\n",
       "  'answer',\n",
       "  'anxious',\n",
       "  'anyone',\n",
       "  'anything',\n",
       "  'apathy',\n",
       "  'appetite',\n",
       "  'applause',\n",
       "  'applying',\n",
       "  'appointed',\n",
       "  'arabia',\n",
       "  'arc',\n",
       "  'archaic',\n",
       "  'around',\n",
       "  'artificial',\n",
       "  'asbestos',\n",
       "  'asia',\n",
       "  'aside',\n",
       "  'ask',\n",
       "  'aspect',\n",
       "  'assassinated',\n",
       "  'assassins',\n",
       "  'assault',\n",
       "  'assembly',\n",
       "  'asserting',\n",
       "  'assistance',\n",
       "  'association',\n",
       "  'attack',\n",
       "  'attacked',\n",
       "  'attention',\n",
       "  'avert',\n",
       "  'away',\n",
       "  'back',\n",
       "  'backgrounds',\n",
       "  'baghdad',\n",
       "  'bahrain',\n",
       "  'bank',\n",
       "  'bankrupt',\n",
       "  'bankruptcy',\n",
       "  'based',\n",
       "  'basic',\n",
       "  'battle',\n",
       "  'become',\n",
       "  'beginning',\n",
       "  'beginnings',\n",
       "  'begun',\n",
       "  'behind',\n",
       "  'belief',\n",
       "  'believe',\n",
       "  'bench',\n",
       "  'beneficiary',\n",
       "  'benefit',\n",
       "  'benefits',\n",
       "  'best',\n",
       "  'better',\n",
       "  'beyond',\n",
       "  'bigger',\n",
       "  'biggest',\n",
       "  'bill',\n",
       "  'billion',\n",
       "  'biological',\n",
       "  'bipartisan',\n",
       "  'birth',\n",
       "  'bless',\n",
       "  'blessed',\n",
       "  'body',\n",
       "  'bombers',\n",
       "  'bonds',\n",
       "  'border',\n",
       "  'born',\n",
       "  'borrowing',\n",
       "  'bottom',\n",
       "  'bought',\n",
       "  'branches',\n",
       "  'break',\n",
       "  'breaux',\n",
       "  'bring',\n",
       "  'brings',\n",
       "  'broader',\n",
       "  'broadly',\n",
       "  'broken',\n",
       "  'brutal',\n",
       "  'budget',\n",
       "  'build',\n",
       "  'burdened',\n",
       "  'burdens',\n",
       "  'bush',\n",
       "  'business',\n",
       "  'businesses',\n",
       "  'buy',\n",
       "  'byron',\n",
       "  'calculated',\n",
       "  'called',\n",
       "  'came',\n",
       "  'can',\n",
       "  'candid',\n",
       "  'capable',\n",
       "  'capital',\n",
       "  'capitol',\n",
       "  'captured',\n",
       "  'car',\n",
       "  'care',\n",
       "  'career',\n",
       "  'careful',\n",
       "  'cars',\n",
       "  'cases',\n",
       "  'casting',\n",
       "  'cells',\n",
       "  'center',\n",
       "  'century',\n",
       "  'certain',\n",
       "  'chance',\n",
       "  'change',\n",
       "  'changed',\n",
       "  'changes',\n",
       "  'changing',\n",
       "  'chaos',\n",
       "  'character',\n",
       "  'check',\n",
       "  'chemical',\n",
       "  'cheney',\n",
       "  'chief',\n",
       "  'child',\n",
       "  'children',\n",
       "  'choice',\n",
       "  'choices',\n",
       "  'choosing',\n",
       "  'chosen',\n",
       "  'circumstances',\n",
       "  'cities',\n",
       "  'citizen',\n",
       "  'citizens',\n",
       "  'civic',\n",
       "  'claims',\n",
       "  'class',\n",
       "  'clean',\n",
       "  'clear',\n",
       "  'clinton',\n",
       "  'closely',\n",
       "  'closes',\n",
       "  'closing',\n",
       "  'coaches',\n",
       "  'coal',\n",
       "  'coalition',\n",
       "  'coalitions',\n",
       "  'code',\n",
       "  'collapsing',\n",
       "  'collection',\n",
       "  'college',\n",
       "  'colleges',\n",
       "  'combination',\n",
       "  'come',\n",
       "  'coming',\n",
       "  'command',\n",
       "  'commanders',\n",
       "  'commit',\n",
       "  'commitment',\n",
       "  'commodity',\n",
       "  'common',\n",
       "  'communism',\n",
       "  'community',\n",
       "  'compassion',\n",
       "  'competent',\n",
       "  'competitive',\n",
       "  'comprehensive',\n",
       "  'concerned',\n",
       "  'conditions',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confront',\n",
       "  'confronted',\n",
       "  'congress',\n",
       "  'congressman',\n",
       "  'consequence',\n",
       "  'conservation',\n",
       "  'conservative',\n",
       "  'constitution',\n",
       "  'constitutional',\n",
       "  'consultation',\n",
       "  'content',\n",
       "  'continue',\n",
       "  'continuing',\n",
       "  'contributions',\n",
       "  'control',\n",
       "  'conviction',\n",
       "  'convince',\n",
       "  'cooperating',\n",
       "  'corporate',\n",
       "  'corps',\n",
       "  'cost',\n",
       "  'costs',\n",
       "  'counsel',\n",
       "  'countries',\n",
       "  'country',\n",
       "  'courage',\n",
       "  'courts',\n",
       "  'coverage',\n",
       "  'created',\n",
       "  'creators',\n",
       "  'credits',\n",
       "  'crime',\n",
       "  'criminals',\n",
       "  'culture',\n",
       "  'cultures',\n",
       "  'cures',\n",
       "  'current',\n",
       "  'cut',\n",
       "  'cuts',\n",
       "  'dad',\n",
       "  'damascus',\n",
       "  'danger',\n",
       "  'dangerous',\n",
       "  'dangers',\n",
       "  'daniel',\n",
       "  'dates',\n",
       "  'day',\n",
       "  'days',\n",
       "  'deal',\n",
       "  'dealers',\n",
       "  'debate',\n",
       "  'decades',\n",
       "  'decency',\n",
       "  'decisions',\n",
       "  'declared',\n",
       "  'declined',\n",
       "  'deepest',\n",
       "  'defeat',\n",
       "  'defend',\n",
       "  'defenders',\n",
       "  'defense',\n",
       "  'defenses',\n",
       "  'deficit',\n",
       "  'defined',\n",
       "  'deliver',\n",
       "  'delivered',\n",
       "  'demand',\n",
       "  'democracies',\n",
       "  'democracy',\n",
       "  'democratic',\n",
       "  'demonstrate',\n",
       "  'deny',\n",
       "  'department',\n",
       "  'departs',\n",
       "  'dependent',\n",
       "  'deposit',\n",
       "  'depriving',\n",
       "  'deserve',\n",
       "  'deserves',\n",
       "  'desire',\n",
       "  'despair',\n",
       "  'destroy',\n",
       "  'destroyed',\n",
       "  'destruction',\n",
       "  'detained',\n",
       "  'detect',\n",
       "  'determined',\n",
       "  'determining',\n",
       "  'developing',\n",
       "  'died',\n",
       "  'differences',\n",
       "  'different',\n",
       "  'dignity',\n",
       "  'diploma',\n",
       "  'direction',\n",
       "  'disabilities',\n",
       "  'disagreed',\n",
       "  'discipline',\n",
       "  'discouraging',\n",
       "  'discretionary',\n",
       "  'discuss',\n",
       "  'disease',\n",
       "  'distant',\n",
       "  'distorted',\n",
       "  'dna',\n",
       "  'doctors',\n",
       "  'dollars',\n",
       "  'done',\n",
       "  'door',\n",
       "  'doubling',\n",
       "  'doubly',\n",
       "  'dramatically',\n",
       "  'drawing',\n",
       "  'dream',\n",
       "  'dreams',\n",
       "  'drug',\n",
       "  'duplicate',\n",
       "  'duty',\n",
       "  'dying',\n",
       "  'dynamic',\n",
       "  'earlier',\n",
       "  'early',\n",
       "  'earn',\n",
       "  'earned',\n",
       "  'earnings',\n",
       "  'easier',\n",
       "  'east',\n",
       "  'easy',\n",
       "  'eaten',\n",
       "  'economic',\n",
       "  'economy',\n",
       "  'edge',\n",
       "  'education',\n",
       "  'effective',\n",
       "  'effort',\n",
       "  'efforts',\n",
       "  'egg',\n",
       "  'egypt',\n",
       "  'elected',\n",
       "  'electing',\n",
       "  'election',\n",
       "  'elections',\n",
       "  'electricity',\n",
       "  'eleven',\n",
       "  'eliminates',\n",
       "  'eliminating',\n",
       "  'else',\n",
       "  'embolden',\n",
       "  'embryos',\n",
       "  'empire',\n",
       "  'employees',\n",
       "  'emptied',\n",
       "  'encourage',\n",
       "  'encourages',\n",
       "  'end',\n",
       "  'ending',\n",
       "  'enemies',\n",
       "  'enemy',\n",
       "  'energy',\n",
       "  'enjoy',\n",
       "  'enough',\n",
       "  'enrichment',\n",
       "  'ensure',\n",
       "  'entering',\n",
       "  'enthusiasm',\n",
       "  'entire',\n",
       "  'entrepreneurs',\n",
       "  'environmentally',\n",
       "  'equal',\n",
       "  'equipment',\n",
       "  'era',\n",
       "  'error',\n",
       "  'especially',\n",
       "  'essential',\n",
       "  'est',\n",
       "  'ethanol',\n",
       "  'ethical',\n",
       "  'europe',\n",
       "  'european',\n",
       "  'eve',\n",
       "  'even',\n",
       "  'evening',\n",
       "  'events',\n",
       "  'eventually',\n",
       "  'ever',\n",
       "  'every',\n",
       "  'everything',\n",
       "  'evidence',\n",
       "  'evil',\n",
       "  'examine',\n",
       "  'example',\n",
       "  'exhausted',\n",
       "  'expand',\n",
       "  'expanded',\n",
       "  'expanding',\n",
       "  'expansion',\n",
       "  'expect',\n",
       "  'experimentation',\n",
       "  'explosions',\n",
       "  'expressed',\n",
       "  'extend',\n",
       "  'extra',\n",
       "  'extremists',\n",
       "  'face',\n",
       "  'failure',\n",
       "  'fair',\n",
       "  'faith',\n",
       "  'faithfully',\n",
       "  'fall',\n",
       "  'fallujah',\n",
       "  'familiar',\n",
       "  'families',\n",
       "  'family',\n",
       "  'farewell',\n",
       "  'fascism',\n",
       "  'fastest',\n",
       "  'father',\n",
       "  'fbi',\n",
       "  'fear',\n",
       "  'february',\n",
       "  'federal',\n",
       "  'feed',\n",
       "  'feels',\n",
       "  'fees',\n",
       "  'fellow',\n",
       "  'fewer',\n",
       "  'fight',\n",
       "  'fighting',\n",
       "  'fill',\n",
       "  'finally',\n",
       "  'financial',\n",
       "  'find',\n",
       "  'fire',\n",
       "  'firefighters',\n",
       "  'firm',\n",
       "  'first',\n",
       "  'fiscally',\n",
       "  'five',\n",
       "  'fix',\n",
       "  'fixing',\n",
       "  'flexible',\n",
       "  'focus',\n",
       "  'focused',\n",
       "  'force',\n",
       "  'forces',\n",
       "  'foreign',\n",
       "  'foreseen',\n",
       "  'forever',\n",
       "  'form',\n",
       "  'former',\n",
       "  'forth',\n",
       "  'forward',\n",
       "  'foundation',\n",
       "  'founders',\n",
       "  'four',\n",
       "  'franklin',\n",
       "  'free',\n",
       "  'freedom',\n",
       "  'freedoms',\n",
       "  'friends',\n",
       "  'frivolous',\n",
       "  'front',\n",
       "  'fueled',\n",
       "  'fulfill',\n",
       "  'fulfilled',\n",
       "  'fund',\n",
       "  'funding',\n",
       "  'funds',\n",
       "  'future',\n",
       "  'gang',\n",
       "  'gangs',\n",
       "  'gap',\n",
       "  'gathers',\n",
       "  'generation',\n",
       "  'generational',\n",
       "  'generations',\n",
       "  'gentlemen',\n",
       "  'george',\n",
       "  'get',\n",
       "  'getting',\n",
       "  'give',\n",
       "  'given',\n",
       "  'gives',\n",
       "  'giving',\n",
       "  'globe',\n",
       "  'go',\n",
       "  'goal',\n",
       "  'god',\n",
       "  'going',\n",
       "  'good',\n",
       "  'got',\n",
       "  'government',\n",
       "  'governments',\n",
       "  'gradual',\n",
       "  'gradually',\n",
       "  'grandchildren',\n",
       "  'grants',\n",
       "  'grateful',\n",
       "  'gray',\n",
       "  'great',\n",
       "  'greater',\n",
       "  'grid',\n",
       "  'ground',\n",
       "  'grounds',\n",
       "  'group',\n",
       "  'groups',\n",
       "  'grow',\n",
       "  'growing',\n",
       "  'grown',\n",
       "  'growth',\n",
       "  'guarantee',\n",
       "  'guest',\n",
       "  'guided',\n",
       "  'guidelines',\n",
       "  'guiding',\n",
       "  'half',\n",
       "  'harbor',\n",
       "  'hardworking',\n",
       "  'harsh',\n",
       "  'hatred',\n",
       "  'headed',\n",
       "  'health',\n",
       "  'healthy',\n",
       "  'hearing',\n",
       "  'hearts',\n",
       "  'held',\n",
       "  'help',\n",
       "  'helping',\n",
       "  'hidden',\n",
       "  'high',\n",
       "  'higher',\n",
       "  'highest',\n",
       "  'history',\n",
       "  'hiv',\n",
       "  'hold',\n",
       "  'holds',\n",
       "  'home',\n",
       "  'homeland',\n",
       "  'homeownership',\n",
       "  'honest',\n",
       "  'honesty',\n",
       "  'honor',\n",
       "  'honored',\n",
       "  'hope',\n",
       "  'hopeful',\n",
       "  'hopes',\n",
       "  'hours',\n",
       "  'however',\n",
       "  'hugged',\n",
       "  'human',\n",
       "  'husband',\n",
       "  'hussein',\n",
       "  'hydrogen',\n",
       "  'idea',\n",
       "  'ideal',\n",
       "  'idealism',\n",
       "  'ideas',\n",
       "  'ideologies',\n",
       "  'immigration',\n",
       "  'impartial',\n",
       "  'imperial',\n",
       "  'important',\n",
       "  'impose',\n",
       "  'improve',\n",
       "  'improved',\n",
       "  'including',\n",
       "  'incoherent',\n",
       "  'income',\n",
       "  'increasing',\n",
       "  'increasingly',\n",
       "  'independent',\n",
       "  'indexing',\n",
       "  'industrialized',\n",
       "  'inflation',\n",
       "  'information',\n",
       "  'initiative',\n",
       "  'injuries',\n",
       "  'innovative',\n",
       "  'inspire',\n",
       "  'instead',\n",
       "  'institutes',\n",
       "  'institution',\n",
       "  'institutions',\n",
       "  'insurance',\n",
       "  'insurgents',\n",
       "  'intelligence',\n",
       "  'intention',\n",
       "  'international',\n",
       "  'interpret',\n",
       "  'interpreter',\n",
       "  'intimidate',\n",
       "  'investment',\n",
       "  'investments',\n",
       "  'invite',\n",
       "  'involves',\n",
       "  'iran',\n",
       "  'iranian',\n",
       "  'iraq',\n",
       "  'iraqi',\n",
       "  'iraqis',\n",
       "  'irresponsible',\n",
       "  'isolated',\n",
       "  'israel',\n",
       "  'issue',\n",
       "  'jail',\n",
       "  'janet',\n",
       "  'jeopardize',\n",
       "  'job',\n",
       "  'jobs',\n",
       "  'john',\n",
       "  'join',\n",
       "  'joint',\n",
       "  'jordan',\n",
       "  'journey',\n",
       "  'judges',\n",
       "  'judicial',\n",
       "  'junk',\n",
       "  'just',\n",
       "  'justice',\n",
       "  'keep',\n",
       "  'kill',\n",
       "  'killed',\n",
       "  'know',\n",
       "  'known',\n",
       "  'knows',\n",
       "  'korea',\n",
       "  'ladies',\n",
       "  'lady',\n",
       "  'landmark',\n",
       "  'large',\n",
       "  'larger',\n",
       "  'last',\n",
       "  'late',\n",
       "  'laughter',\n",
       "  'laura',\n",
       "  'law',\n",
       "  'laws',\n",
       "  'lawsuits',\n",
       "  'lawyers',\n",
       "  'lead',\n",
       "  'leader',\n",
       "  'leaders',\n",
       "  'leadership',\n",
       "  'leading',\n",
       "  'leads',\n",
       "  'leave',\n",
       "  'leaving',\n",
       "  'lebanon',\n",
       "  'left',\n",
       "  'legal',\n",
       "  'legislate',\n",
       "  'legislation',\n",
       "  'less',\n",
       "  'let',\n",
       "  'lets',\n",
       "  'letter',\n",
       "  'level',\n",
       "  'liability',\n",
       "  'liberation',\n",
       "  'liberty',\n",
       "  'life',\n",
       "  'lift',\n",
       "  'like',\n",
       "  'limiting',\n",
       "  'limits',\n",
       "  'line',\n",
       "  'listen',\n",
       "  'literacy',\n",
       "  'little',\n",
       "  'live',\n",
       "  'lives',\n",
       "  'living',\n",
       "  'll',\n",
       "  'long',\n",
       "  'longer',\n",
       "  'losing',\n",
       "  'lot',\n",
       "  'loved',\n",
       "  'low',\n",
       "  'lower',\n",
       "  'loyalty',\n",
       "  'm',\n",
       "  'main',\n",
       "  'major',\n",
       "  'make',\n",
       "  'makes',\n",
       "  'making',\n",
       "  'manhood',\n",
       "  'many',\n",
       "  'marine',\n",
       "  'market',\n",
       "  'markets',\n",
       "  'marriage',\n",
       "  'marshals',\n",
       "  'mass',\n",
       "  'massive',\n",
       "  'materials',\n",
       "  'matter',\n",
       "  'may',\n",
       "  'measured',\n",
       "  'measures',\n",
       "  'medical',\n",
       "  'medicine',\n",
       "  'meet',\n",
       "  'meetings',\n",
       "  'members',\n",
       "  'memory',\n",
       "  'men',\n",
       "  'message',\n",
       "  'middle',\n",
       "  'might',\n",
       "  'military',\n",
       "  'million',\n",
       "  'millions',\n",
       "  'mind',\n",
       "  'minister',\n",
       "  'minorities',\n",
       "  'minority',\n",
       "  'mirror',\n",
       "  'mislead',\n",
       "  'mission',\n",
       "  'mix',\n",
       "  'modernized',\n",
       "  'mom',\n",
       "  'momentum',\n",
       "  'money',\n",
       "  'months',\n",
       "  'moral',\n",
       "  'morning',\n",
       "  'morocco',\n",
       "  'mortar',\n",
       "  'move',\n",
       "  'movements',\n",
       "  'moving',\n",
       "  'moynihan',\n",
       "  'mr',\n",
       "  'much',\n",
       "  'murder',\n",
       "  'must',\n",
       "  'myth',\n",
       "  'name',\n",
       "  'nation',\n",
       "  'national',\n",
       "  'nations',\n",
       "  'nationwide',\n",
       "  'nato',\n",
       "  'natural',\n",
       "  'nearing',\n",
       "  'nearly',\n",
       "  'need',\n",
       "  'needed',\n",
       "  'needless',\n",
       "  'needs',\n",
       "  'neighbors',\n",
       "  'nest',\n",
       "  'network',\n",
       "  'never',\n",
       "  'new',\n",
       "  'newly',\n",
       "  'next',\n",
       "  'nine',\n",
       "  'nominate',\n",
       "  'nominee',\n",
       "  'none',\n",
       "  'north',\n",
       "  'norwood',\n",
       "  'now',\n",
       "  'nuclear',\n",
       "  'number',\n",
       "  'occupation',\n",
       "  'occupied',\n",
       "  'occurred',\n",
       "  'offensive',\n",
       "  'offer',\n",
       "  'office',\n",
       "  'officers',\n",
       "  'often',\n",
       "  'old',\n",
       "  'older',\n",
       "  'one',\n",
       "  'open',\n",
       "  'opened',\n",
       "  'opens',\n",
       "  'opportunities',\n",
       "  'opportunity',\n",
       "  'opposed',\n",
       "  'oppression',\n",
       "  'options',\n",
       "  'orders',\n",
       "  'ordinary',\n",
       "  'organizations',\n",
       "  'others',\n",
       "  'outcome',\n",
       "  'outdated',\n",
       "  'outreach',\n",
       "  'overcome',\n",
       "  'overturn',\n",
       "  'ownership',\n",
       "  'p',\n",
       "  'paid',\n",
       "  'pakistan',\n",
       "  'palestine',\n",
       "  'palestinian',\n",
       "  'palestinians',\n",
       "  'panel',\n",
       "  'parent',\n",
       "  'parents',\n",
       "  'part',\n",
       "  'partisan',\n",
       "  'partners',\n",
       "  'parts',\n",
       "  'pass',\n",
       "  'passed',\n",
       "  'passing',\n",
       "  'past',\n",
       "  'pastors',\n",
       "  'path',\n",
       "  'patients',\n",
       "  'patrick',\n",
       "  'patterns',\n",
       "  'pay',\n",
       "  'paycheck',\n",
       "  'paychecks',\n",
       "  'paying',\n",
       "  'payroll',\n",
       "  'pays',\n",
       "  'peace',\n",
       "  'peaceful',\n",
       "  'pell',\n",
       "  'penny',\n",
       "  'people',\n",
       "  'per',\n",
       "  'percentage',\n",
       "  'permanent',\n",
       "  'permanently',\n",
       "  'permits',\n",
       "  'permitting',\n",
       "  'person',\n",
       "  'personal',\n",
       "  'pflugerville',\n",
       "  'phase',\n",
       "  'placed',\n",
       "  'places',\n",
       "  'plan',\n",
       "  'plans',\n",
       "  'plant',\n",
       "  'plutonium',\n",
       "  'points',\n",
       "  'police',\n",
       "  'policies',\n",
       "  'policy',\n",
       "  'political',\n",
       "  'politics',\n",
       "  'polls',\n",
       "  'pollution',\n",
       "  'poor',\n",
       "  'portion',\n",
       "  'possibility',\n",
       "  'possible',\n",
       "  'posts',\n",
       "  'power',\n",
       "  'powerful',\n",
       "  'predecessor',\n",
       "  'prepare',\n",
       "  'preserve',\n",
       "  'president',\n",
       "  'prevent',\n",
       "  'preventing',\n",
       "  'prevention',\n",
       "  'prices',\n",
       "  'primary',\n",
       "  'prime',\n",
       "  'principle',\n",
       "  'principles',\n",
       "  'priorities',\n",
       "  'privilege',\n",
       "  'pro',\n",
       "  'problems',\n",
       "  'production',\n",
       "  'productive',\n",
       "  'program',\n",
       "  'programs',\n",
       "  'progress',\n",
       "  'proliferation',\n",
       "  'promise',\n",
       "  'promote',\n",
       "  'proposal',\n",
       "  'proposals',\n",
       "  'propose',\n",
       "  'prosecuted',\n",
       "  'prosperity',\n",
       "  'protect',\n",
       "  'protected',\n",
       "  'proud',\n",
       "  'provide',\n",
       "  'provided',\n",
       "  'providence',\n",
       "  'provides',\n",
       "  'providing',\n",
       "  'punish',\n",
       "  'purchased',\n",
       "  'purpose',\n",
       "  'purposes',\n",
       "  'pursue',\n",
       "  'pursuing',\n",
       "  'qaeda',\n",
       "  'qualified',\n",
       "  'question',\n",
       "  'quickly',\n",
       "  'races',\n",
       "  'radicalism',\n",
       "  'raised',\n",
       "  'raising',\n",
       "  'ranging',\n",
       "  'rate',\n",
       "  'rates',\n",
       "  'rather',\n",
       "  're',\n",
       "  'reach',\n",
       "  'reaffirmed',\n",
       "  'real',\n",
       "  'reauthorize',\n",
       "  'receive',\n",
       "  'recently',\n",
       "  'recession',\n",
       "  'recognize',\n",
       "  'recommendation',\n",
       "  ...],\n",
       " 1]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_word_matrix(test, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
