{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-words\n",
      "  Downloading stop-words-2015.2.23.1.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/annayanchenko/Library/Caches/pip/wheels/f3/bf/61/6eda06c4c033618410aa68ee948bfbf2f503f2b2a955ddabcb\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2015.2.23.1\n",
      "\u001b[33mYou are using pip version 8.0.3, however version 8.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus):\n",
    "    D = len(corpus)\n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "   \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        #Remove punctuation \n",
    "        text = docs[i].translate(table)\n",
    "        #Splits string by blankspace and goes to lower case#\n",
    "        words = text.lower().split()\n",
    "    \n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        \n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "        \n",
    "        #Find number of unique words in each document#\n",
    "        V = len(set(text))\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = Vwords.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs)\n",
    "corpusMatrix[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Megan Robertson\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:294: FutureWarning: in the future, full(5, 1) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#lengths and number of topics will go here\n",
    "\n",
    "k = 5\n",
    "D = 4\n",
    "tol = 1\n",
    "\n",
    "#need to define number of documents and number of topics here, \n",
    "\n",
    "#initialize phi and gamma\n",
    "alpha = np.full(shape = k, fill_value = 1)\n",
    "phi = np.full(shape = (N,k), fill_value = 1/k)\n",
    "beta = np.full(shape = (k, V), fill_value = 1/k)\n",
    "gamma = alpha + N/k\n",
    "converge = 0 \n",
    "\n",
    "#looping through the number of documents\n",
    "for d in range(0,D): #D is the number of documents\n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    while converge == 0:\n",
    "    \n",
    "        #creating a place to store the updated phi\n",
    "        newPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*scipy.special.psi(gamma[i])\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1) #normalizing the rows of new phi\n",
    "        newGamma = alpha + np.sum(newPhi) #updating gamma\n",
    "        \n",
    "        #checking for convergence\n",
    "        if np.linalg.norm(newGamma - gamma) + np.linalg.norm(newPhi - phi) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
