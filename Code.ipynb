{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in c:\\users\\megan robertson\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.0.3, however version 8.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 The quick brown fox\n",
      "s2 Brown fox jumps over the jumps jumps jumps\n",
      "s3 The the the lazy dog elephant.\n",
      "s4 The the the the the dog peacock lion tiger elephant\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4\n",
    "for k, v in docs.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus):\n",
    "    D = len(corpus)\n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "   \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        #Remove punctuation \n",
    "        text = docs[i].translate(table)\n",
    "        #Splits string by blankspace and goes to lower case#\n",
    "        words = text.lower().split()\n",
    "    \n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        \n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "        \n",
    "        #Find number of unique words in each document#\n",
    "        V = len(set(text))\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = Vwords.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs)\n",
    "corpusMatrix[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  1.,  0.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [ 1.,  0.,  0.]]), array([[ 0.,  0.,  1.],\n",
       "         [ 1.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.],\n",
       "         [ 0.,  1.,  0.],\n",
       "         [ 0.,  1.,  0.],\n",
       "         [ 0.,  1.,  0.]]), array([[ 0.,  1.,  0.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [ 1.,  0.,  0.]]), array([[ 0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.]])],\n",
       " [['fox', 'quick', 'brown'],\n",
       "  ['fox', 'jumps', 'brown'],\n",
       "  ['elephant', 'lazy', 'dog'],\n",
       "  ['elephant', 'lion', 'dog', 'peacock', 'tiger']]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  The output of this function are the matricies gamma and phi, where gamma (k vector) is the Dirichlet paramteters and the matrix phi (N x k, where k is the number of topics) are the multinomial paramters.  See page 1004 of paper for derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    phi = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        #creating a place to store the updated phi\n",
    "        newPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*scipy.special.psi(gamma[i])\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "        newGamma = alpha + np.sum(newPhi) #updating gamma\n",
    "        \n",
    "        #checking for convergence\n",
    "        if np.linalg.norm(newGamma - gamma) + np.linalg.norm(newPhi - phi) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            gamma = newGamma\n",
    "            phi = newPhi\n",
    "            count = count +1\n",
    "    return (newPhi, newGamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 for derivation.  \n",
    "\n",
    "The alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the Mstep() function maximizes for alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "#Following derivation in #\n",
    "#http://arxiv.org/pdf/1405.0099.pdf\n",
    "def alphaUpdate(alphaOld, phiMat, tol):\n",
    "    N = phiMat.shape[0]\n",
    "    K = phiMat.shape[1]\n",
    "    x = np.zeros(shape = (K, 1))\n",
    "    d = np.zeros(shape = (K, 1))\n",
    "    v = np.zeros(shape = (K, 1))\n",
    "    g = np.zeros(shape = (K, 1))\n",
    "    alphaNew = np.zeros(shape = (K, 1))\n",
    "    alphaOld = alphaOld\n",
    "    converge = 0\n",
    "    while converge == 0: \n",
    "        Z = 0\n",
    "        for k in range(0, K):\n",
    "            d[k] = -scipy.special.polygamma(1, alphaOld[k])\n",
    "            v[k] = 1/N * sum(np.log(phiMat[:, k]))\n",
    "            g[k] = scipy.special.psi(sum(alphaOld)) - scipy.special.psi(alphaOld[k]) + v[k]\n",
    "            x[k] = g[k] - alphaOld[k]*d[k]\n",
    "            Z = Z + alphaOld[k]/x[k]\n",
    "        c = scipy.special.polygamma(1,sum(alphaOld))\n",
    "        Z = Z*c\n",
    "        S = 0\n",
    "        for k in range(0, K):\n",
    "            S = S + 1/((1+Z)/x[k])\n",
    "        for k in range(0, K):\n",
    "            alphaNew[k] = S + x[k]*(1-c*alphaOld[k]*S)\n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            alphaOld = alphaNew\n",
    "    return alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.62674189],\n",
       "       [ 6.59584822],\n",
       "       [ 4.63433397]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphaUpdate(np.array([[2], [3], [0.1]]), phi1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mstep(k, d, phi, alpha, corpusMatrix, tol):\n",
    "    #Calculate beta#\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    beta = np.zeros(shape = (k,V))\n",
    "    for i in range(0,k):\n",
    "        for n in range(0,V):\n",
    "            beta[i,n] = np.sum(phi[:,i][:, np.newaxis]*(corpusMatrix[0][d][n,:])[np.newaxis, :])\n",
    "    #Normalize the columns of beta#\n",
    "    beta = beta/np.sum(beta, axis = 0)[np.newaxis, :]\n",
    "    \n",
    "    \n",
    "    ##Update ALPHA##\n",
    "    alphaNew = alphaUpdate(alpha, phi, tol)\n",
    "    return(alphaNew, beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Function:**\n",
    "Finally, we implement the entire Latent Dirichlet Allocation method in the LDA function, which takes as its arguments k (the number of topics), D (the number of documents in the corpus), a corpus matrix (the output from make_word_matrix above) and a tolerance (which sets the convergence criteria for the while loops).  For each document d, the function runs until the alpha or beta parameters converge, by first running the E step and then the M step for each document separately.  The final values of phi, gamma, alpha and beta are returned for all D documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, D, corpusMatrix, tol):\n",
    "\n",
    "    \n",
    "    output = []\n",
    "    #looping through the number of documents\n",
    "    for d in range(0,D): #D is the number of documents\n",
    "        converge = 0\n",
    "        #initialize alpha and beta for first iteration\n",
    "        alphaOld = np.random.rand(k)\n",
    "        V = corpusMatrix[0][d].shape[1]\n",
    "        betaOld = np.random.rand(k, V)\n",
    "        betaOld = betaOld/np.sum(betaOld, axis = 0)[np.newaxis, :]\n",
    "        while converge == 0:\n",
    "            phi, gamma = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            alphaNew, betaNew = Mstep(k, d, phi, alphaOld, corpusMatrix, tol)\n",
    "            if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "                converge =1\n",
    "            else: \n",
    "                converge =0\n",
    "                alphaOld = alphaNew\n",
    "                betaOld = betaNew\n",
    "        output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.03483214,  0.07816957,  0.35017751,  0.51974233,  0.01707844],\n",
       "         [ 0.29631511,  0.06611542,  0.21852462,  0.2524068 ,  0.16663805],\n",
       "         [ 0.02319167,  0.210271  ,  0.41371737,  0.03014734,  0.32267263]]),\n",
       "  array([ 3.38762235,  3.60110457,  3.52779709,  3.91419835,  3.43261474]),\n",
       "  array([[ 2.57894629],\n",
       "         [ 2.87665821],\n",
       "         [ 3.69103689],\n",
       "         [ 3.18729268],\n",
       "         [ 2.79036246]]),\n",
       "  array([[ 0.11811297,  0.11811297,  0.11811297],\n",
       "         [ 0.11818533,  0.11818533,  0.11818533],\n",
       "         [ 0.32747317,  0.32747317,  0.32747317],\n",
       "         [ 0.26743216,  0.26743216,  0.26743216],\n",
       "         [ 0.16879637,  0.16879637,  0.16879637]])],\n",
       " [array([[ 0.27631751,  0.26150871,  0.18809073,  0.04576364,  0.22831941],\n",
       "         [ 0.26700645,  0.11331318,  0.24516055,  0.25857075,  0.11594907],\n",
       "         [ 0.28611152,  0.06100732,  0.24779865,  0.10024051,  0.30484201],\n",
       "         [ 0.28611152,  0.06100732,  0.24779865,  0.10024051,  0.30484201],\n",
       "         [ 0.28611152,  0.06100732,  0.24779865,  0.10024051,  0.30484201],\n",
       "         [ 0.28611152,  0.06100732,  0.24779865,  0.10024051,  0.30484201]]),\n",
       "  array([ 6.68090512,  6.73581049,  6.60388204,  6.14944232,  6.24114898]),\n",
       "  array([[ 2.11932348],\n",
       "         [-0.32541346],\n",
       "         [ 1.69733819],\n",
       "         [-0.41822961],\n",
       "         [ 1.09714213]]),\n",
       "  array([[ 0.28129501,  0.28129501,  0.28129501],\n",
       "         [ 0.10314186,  0.10314186,  0.10314186],\n",
       "         [ 0.23740764,  0.23740764,  0.23740764],\n",
       "         [ 0.1175494 ,  0.1175494 ,  0.1175494 ],\n",
       "         [ 0.26060609,  0.26060609,  0.26060609]])],\n",
       " [array([[ 0.06299026,  0.5153841 ,  0.1765411 ,  0.17002676,  0.07505777],\n",
       "         [ 0.29727711,  0.15689124,  0.17248726,  0.1700033 ,  0.2033411 ],\n",
       "         [ 0.1541653 ,  0.09171837,  0.24045183,  0.3521085 ,  0.161556  ]]),\n",
       "  array([ 3.38108086,  3.42940357,  3.13915564,  3.0727331 ,  3.11029791]),\n",
       "  array([[  5.92472590e+08],\n",
       "         [  1.49427881e+08],\n",
       "         [  3.73920904e+08],\n",
       "         [  5.11782876e+08],\n",
       "         [  9.38356399e+08]]),\n",
       "  array([[ 0.17147756,  0.17147756,  0.17147756],\n",
       "         [ 0.25466457,  0.25466457,  0.25466457],\n",
       "         [ 0.1964934 ,  0.1964934 ,  0.1964934 ],\n",
       "         [ 0.23071285,  0.23071285,  0.23071285],\n",
       "         [ 0.14665162,  0.14665162,  0.14665162]])],\n",
       " [array([[ 0.11301003,  0.20975942,  0.03791899,  0.17013629,  0.46917528],\n",
       "         [ 0.26677028,  0.24193484,  0.13379729,  0.17832656,  0.17917103],\n",
       "         [ 0.16299789,  0.30871046,  0.19916064,  0.12936484,  0.19976617],\n",
       "         [ 0.07630873,  0.04865007,  0.04254305,  0.4298247 ,  0.40267345],\n",
       "         [ 0.2992491 ,  0.05908   ,  0.29675458,  0.18600327,  0.15891306]]),\n",
       "  array([ 5.49605573,  5.48648213,  5.32964859,  5.5689318 ,  5.40316833]),\n",
       "  array([[ 2.45835597],\n",
       "         [ 2.23919023],\n",
       "         [ 1.93569493],\n",
       "         [ 2.77203025],\n",
       "         [ 2.93504422]]),\n",
       "  array([[ 0.1836672 ,  0.1836672 ,  0.1836672 ,  0.1836672 ,  0.1836672 ],\n",
       "         [ 0.17362696,  0.17362696,  0.17362696,  0.17362696,  0.17362696],\n",
       "         [ 0.14203491,  0.14203491,  0.14203491,  0.14203491,  0.14203491],\n",
       "         [ 0.21873113,  0.21873113,  0.21873113,  0.21873113,  0.21873113],\n",
       "         [ 0.2819398 ,  0.2819398 ,  0.2819398 ,  0.2819398 ,  0.2819398 ]])]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA(5, 4, corpusMatrix, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps\n",
    "\n",
    "- Compare output to Python package\n",
    "- Test on corpus in paper\n",
    "- Model topics on different corpus\n",
    "- Time and optimize (use Cython, quite slow now)\n",
    "- Run collaborative filtering\n",
    "- Compare to Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
