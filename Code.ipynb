{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in c:\\users\\megan robertson\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.0.3, however version 8.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 The quick brown fox\n",
      "s2 Brown fox jumps over the jumps jumps jumps\n",
      "s3 The the the lazy dog elephant.\n",
      "s4 The the the the the dog peacock lion tiger elephant\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4 \n",
    "for k, v in docs.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test = collections.OrderedDict()\n",
    "for i in docs:\n",
    "    print(isinstance(i, str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "    \n",
    "    #Define stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    \n",
    "    \n",
    "    #Initialize stemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    \n",
    "    #Check to make sure that dictionary isn't empty#\n",
    "    if M ==0:\n",
    "        print(\"Input dictionary is empty\")\n",
    "        return;\n",
    "    \n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        if isinstance(i, str) != True:\n",
    "            print(\"Corpus input is not a string\")\n",
    "            return;\n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        # stem tokens\n",
    "        text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "            #Remove stop words#\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "            \n",
    "            #remove stop words\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = wordOrder.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'eleph',\n",
       "  'fox',\n",
       "  'jump',\n",
       "  'lazi',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs, 1)\n",
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  The output of this function are the matricies gamma and phi, where gamma (k vector) is the Dirichlet paramteters and the matrix phi (N x k, where k is the number of topics) are the multinomial paramters.  See page 1004 of paper for derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    newPhi = oldPhi\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,k))\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*np.exp(scipy.special.psi(gamma[i]))\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        for i in range(0,k):\n",
    "            gamma[i] = alpha[i] + np.sum(newPhi[:, i]) #updating gamma\n",
    "\n",
    "\n",
    "        criteria = (1/(N*k)*np.sum((newPhi - oldPhi)**2))**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 for derivation.  \n",
    "\n",
    "The alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the Mstep() function maximizes for alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "def alphaUpdate(k, M, alphaOld, gamma, tol):\n",
    "    h = np.zeros(k)\n",
    "    g = np.zeros(k)\n",
    "    alphaNew = np.zeros(k)\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(0, M):\n",
    "                docSum += scipy.special.psi(gamma[d][i]) - scipy.special.psi(np.sum(gamma[d]))\n",
    "            g[i] = M*(scipy.special.psi(sum(alphaOld)) - scipy.special.psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*scipy.special.polygamma(1, alphaOld[i])\n",
    "        z =  -scipy.special.polygamma(1, np.sum(alphaOld))\n",
    "        c = np.sum(g/h)/(1/z + np.sum(1/h))\n",
    "        step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            alphaNew = alphaOld + step\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol):\n",
    "    #Calculate beta#\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(0,M):\n",
    "                Nd = corpusMatrix[0][d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi[d][n,i]*corpusMatrix[0][d][n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    ##Update ALPHA##\n",
    "    alphaNew = alphaUpdate(k, M, alphaOld, gamma, tol)\n",
    "    return(alphaNew, beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Function:**\n",
    "Finally, we implement the entire Latent Dirichlet Allocation method in the LDA function, which takes as its arguments k (the number of topics), D (the number of documents in the corpus), a corpus matrix (the output from make_word_matrix above) and a tolerance (which sets the convergence criteria for the while loops).  For each document d, the function runs until the alpha or beta parameters converge, by first running the E step and then the M step for each document separately.  The final values of phi, gamma, alpha and beta are returned for all D documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, corpusMatrix, tol, needToSplit):\n",
    "\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    if needToSplit != 0 and needToSplit != 1:\n",
    "        print(\"NeedToSplit argument must be 0 or 1\")\n",
    "        return;\n",
    "    \n",
    "    \n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = 10*np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        alphaNew, betaNew = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[array([[ 0.53501824,  0.11374731,  0.35123445],\n",
       "          [ 0.19061895,  0.35207861,  0.45730244],\n",
       "          [ 0.35818844,  0.35335088,  0.28846068]]),\n",
       "   array([[ 0.18513865,  0.36526525,  0.4495961 ],\n",
       "          [ 0.34856137,  0.36729211,  0.28414652],\n",
       "          [ 0.32913538,  0.37874029,  0.29212433],\n",
       "          [ 0.32913538,  0.37874029,  0.29212433],\n",
       "          [ 0.32913538,  0.37874029,  0.29212433],\n",
       "          [ 0.32913538,  0.37874029,  0.29212433]]),\n",
       "   array([[ 0.60092832,  0.01619579,  0.38287589],\n",
       "          [ 0.4445552 ,  0.25094259,  0.30450221],\n",
       "          [ 0.64304264,  0.21680562,  0.14015174]]),\n",
       "   array([[ 0.42098608,  0.26314456,  0.31586935],\n",
       "          [ 0.06402201,  0.45298396,  0.48299403],\n",
       "          [ 0.52462091,  0.2017817 ,  0.27359739],\n",
       "          [ 0.26213644,  0.15401415,  0.58384941],\n",
       "          [ 0.62031341,  0.23159005,  0.14809654]])],\n",
       "  [array([ 9.09032941,  6.81132078,  8.05666184]),\n",
       "   array([ 9.85674531,  8.2396625 ,  8.86190421]),\n",
       "   array([ 9.69502994,  6.47608797,  7.78719412]),\n",
       "   array([ 9.89858263,  7.29565841,  8.76407099])],\n",
       "  array([ 8.22689989,  6.14772074,  7.14716834]),\n",
       "  array([[ 0.05767867,  0.1328603 ,  0.19392473,  0.10848586,  0.20208868,\n",
       "           0.0922423 ,  0.08052913,  0.00982736,  0.08212512,  0.04023786],\n",
       "         [ 0.14777939,  0.10590665,  0.0923736 ,  0.14845904,  0.31209583,\n",
       "           0.00333648,  0.04156887,  0.09331883,  0.02343298,  0.03172832],\n",
       "         [ 0.16104964,  0.11016736,  0.05118795,  0.10168523,  0.2075051 ,\n",
       "           0.0679922 ,  0.04858621,  0.08577146,  0.06237322,  0.10368165]])]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA(3, corpusMatrix, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to return the most probable words\n",
    "#p is the number of words you want returned for each topic\n",
    "def mostCommon(beta, wordList, p):\n",
    "    k = beta.shape[0]\n",
    "    topicWords = []\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    for i in range(0, k):\n",
    "        document = betaDF.loc[i,:]\n",
    "        document.sort(1, ascending = 0)\n",
    "        mostCommon = pd.DataFrame(document[0:p])\n",
    "        topicWords.append(mostCommon)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\n",
      "s2 My mother spends a lot of time driving my brother around to baseball practice.\n",
      "s3 Some health experts suggest that driving may cause increased tension and blood pressure.\n",
      "s4 I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\n",
      "s5 Health professionals say that brocolli is good for your health.\n"
     ]
    }
   ],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "docsTest = collections.OrderedDict()\n",
    "docsTest[\"s1\"] = doc_a\n",
    "docsTest[\"s2\"] = doc_b\n",
    "docsTest[\"s3\"] = doc_c\n",
    "docsTest[\"s4\"] = doc_d \n",
    "docsTest[\"s5\"] = doc_e \n",
    "for k, v in docsTest.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cTest = make_word_matrix(docsTest, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Union Example**\n",
    "\n",
    "In order to determine the initial performance time of our algorithm, we implemented LDA on a small corpus of eight documents. These documents are each of the State of the Union Adresses delivered by President Clinton during his time in office. These were found in the nltk Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"state_union\")\n",
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a diciontary of the state of the Union Addresses for Clinton\n",
    "fileNames = state_union.fileids()\n",
    "Clinton = fileNames[50:54]\n",
    "\n",
    "ClintonSOTU = {}\n",
    "for i in range(0, len(Clinton)):\n",
    "    ClintonSOTU[Clinton[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(Clinton[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#should we include the construction of the corpus matrix in LDA function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 202 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(7)\n",
    "ClintCorp = make_word_matrix(ClintonSOTU, 0)\n",
    "Results = LDA(3, ClintCorp, 0.1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annayanchenko/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[                 0\n",
       " year      0.027425\n",
       " peopl     0.018757\n",
       " will      0.015316\n",
       " can       0.014768\n",
       " us        0.011999\n",
       " congress  0.010766\n",
       " govern    0.010200\n",
       " american  0.009560\n",
       " let       0.008880\n",
       " give      0.008825,                  1\n",
       " work      0.028984\n",
       " peopl     0.018815\n",
       " s         0.017864\n",
       " american  0.016418\n",
       " can       0.012685\n",
       " america   0.011470\n",
       " will      0.010532\n",
       " t         0.010124\n",
       " time      0.009811\n",
       " tax       0.009292,                 2\n",
       " must     0.015145\n",
       " s        0.013888\n",
       " will     0.011184\n",
       " world    0.010891\n",
       " way      0.009042\n",
       " help     0.008930\n",
       " deficit  0.008924\n",
       " famili   0.008755\n",
       " need     0.008169\n",
       " say      0.007506]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommon(Results[0][3], ClintCorp[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating profile files for the construction of the word matrix and LDA for SOTU example\n",
    "%prun -q -D ClintonWordMatrix.prof make_word_matrix(ClintonSOTU,0)\n",
    "%prun -q -D ClintonLDA.prof LDA(3, ClintCorp, 0.1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#printing profile results\n",
    "import pstats\n",
    "wordMatrixProfile = pstats.Stats('ClintonWordMatrix.prof')\n",
    "LDAProfile = pstats.Stats('ClintonLDA.prof')\n",
    "wordMatrixProfile.print_stats()\n",
    "LDAProfile.print_stats()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movie Prediction** \n",
    "\n",
    "In addition to text analysis, LDA can be implemented for other problems that have the same structure as a text corpora. In this section, we use LDA in order to predict movies preferred by users of the site MovieLens.com. The goal is to predict another movie that the user likes based on the movies that they have already said that they prefer. The definition of preferred here is that the user rated the movie four or five, where five is the maxiumum possible rating. Users who gave a preferred rating to at least 50 movies were used. A training set of 2,535 users was used to fit the model. The model is then shown all but one of the preferred movies for the users in the test data, and this is used to determine the last movie that the user preferred. Thus, it is possible to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "import pandas as pd\n",
    "trainRatings = pd.read_csv(\"UserRatingsTraining.csv\")\n",
    "testRatings = pd.read_csv(\"UserRatingsTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#making a dictionary for the training and testing data\n",
    "trainingUsers = list(set(trainRatings['userID']))\n",
    "trainingDict = {}\n",
    "for i in range(0, len(trainingUsers)):\n",
    "    movies = list(map(str, list(pd.DataFrame(trainRatings[trainRatings['userID'] == trainingUsers[0]])['movieID'])))\n",
    "    trainingDict[trainingUsers[i]] = movies\n",
    "    \n",
    "testUsers = list(set(testRatings['userID']))\n",
    "testDict = {}\n",
    "for i in range(0, len(testUsers)):\n",
    "    movies = list(map(str, list(pd.DataFrame(testRatings[testRatings['userID'] == testUsers[0]])['movieID'])))\n",
    "    testDict[testUsers[i]] = movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#running LDA analysis to get parameters for movie training data\n",
    "movieMatrix = make_word_matrix(trainingDict, 0)\n",
    "movieOutput = LDA(3, 4, movieMatrix, 0.1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the above parameters to make predictions for the test set\n",
    "#hold out one movie randomly, then determine the likelihood of the possible held out movies\n",
    "#the highest likelihood should be assigned to the last movie, if the model works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps\n",
    "\n",
    "- Compare output to Python package\n",
    "- Test on corpus in paper\n",
    "- Model topics on different corpus\n",
    "- Time and optimize (use Cython, quite slow now)\n",
    "- Run collaborative filtering\n",
    "- Compare to Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare our output to Python Package\n",
    "\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "for i in docs:\n",
    "    stemmed_tokens = p_stemmer.stem(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"The quick brown fox\"\n",
    "doc_b = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "doc_c =  \"The the the lazy dog elephant.\"\n",
    "doc_d = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, iterations = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.208*jump + 0.122*fox + 0.106*brown + 0.105*dog'), (1, '0.199*jump + 0.128*brown + 0.104*fox + 0.103*eleph'), (2, '0.148*jump + 0.133*dog + 0.131*eleph + 0.108*fox')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta =  LDATest(3, corpusMatrix, 0.1, 1)[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annayanchenko/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[              0\n",
       " jump   0.502213\n",
       " dog    0.233400\n",
       " brown  0.073339\n",
       " lion   0.050304,               1\n",
       " fox    0.180265\n",
       " jump   0.163142\n",
       " brown  0.137126\n",
       " dog    0.135596,                 2\n",
       " jump     0.239419\n",
       " eleph    0.143304\n",
       " peacock  0.140478\n",
       " lion     0.133684]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommon(beta, corpusMatrix[1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown',\n",
       " 'dog',\n",
       " 'eleph',\n",
       " 'fox',\n",
       " 'jump',\n",
       " 'lazi',\n",
       " 'lion',\n",
       " 'peacock',\n",
       " 'quick',\n",
       " 'tiger']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.62560896e-34,   3.33343130e-01,   3.33343094e-01,\n",
       "          3.30440861e-34,   3.27807481e-36,   3.33313776e-01,\n",
       "          1.55467573e-14,   1.53680851e-14,   1.26552889e-33,\n",
       "          5.71817087e-15],\n",
       "       [  1.09742914e-54,   1.99989427e-01,   1.99989449e-01,\n",
       "          1.97699403e-54,   1.67679702e-57,   1.84020254e-56,\n",
       "          2.00007041e-01,   2.00007041e-01,   2.83838287e-51,\n",
       "          2.00007041e-01],\n",
       "       [  2.22222222e-01,   4.02204997e-33,   1.14123694e-32,\n",
       "          2.22222222e-01,   4.44444444e-01,   1.02851237e-35,\n",
       "          2.11185563e-38,   1.71351548e-38,   1.11111111e-01,\n",
       "          9.35199488e-40]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDATest(k, corpusMatrix, tol, needToSplit):\n",
    "\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    if needToSplit != 0 and needToSplit != 1:\n",
    "        print(\"NeedToSplit argument must be 0 or 1\")\n",
    "        return;\n",
    "    \n",
    "    \n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    iterations = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = 10*np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "\n",
    "    phi = []\n",
    "    gamma = []\n",
    "    #looping through the number of documents\n",
    "    for d in range(0,M): #M is the number of documents\n",
    "        phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "        phi.append(phiT)\n",
    "        gamma.append(gammaT)\n",
    "\n",
    "    alphaNew, betaNew = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)\n",
    "\n",
    "\n",
    "    converge =0\n",
    "    alphaOld = alphaNew\n",
    "    betaOld = betaNew\n",
    "    iterations += iterations\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
