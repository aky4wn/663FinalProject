{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing the stop words package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in c:\\users\\megan robertson\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.0.3, however version 8.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#uncomment if you do not have the following package\n",
    "#! pip install stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recreating the toy example from Homework 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 The quick brown fox\n",
      "s2 Brown fox jumps over the jumps jumps jumps\n",
      "s3 The the the lazy dog elephant.\n",
      "s4 The the the the the dog peacock lion tiger elephant\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4 \n",
    "for k, v in docs.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "    \n",
    "    #Define stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    \n",
    "    \n",
    "    #Initialize stemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    \n",
    "    #Check to make sure NeedToSplit argument is 0 or 1\n",
    "    if needToSplit != 0 and needToSplit != 1:\n",
    "        print(\"NeedToSplit argument must be 0 or 1\")\n",
    "        return;\n",
    "    \n",
    "    #Check to make sure that dictionary isn't empty#\n",
    "    if M ==0:\n",
    "        print(\"Input dictionary is empty\")\n",
    "        return;\n",
    "    \n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        if isinstance(i, str) != True:\n",
    "            print(\"Corpus input is not a string\")\n",
    "            return;\n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        # stem tokens\n",
    "        text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "            #Remove stop words#\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "            \n",
    "            #remove stop words\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = wordOrder.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'eleph',\n",
       "  'fox',\n",
       "  'jump',\n",
       "  'lazi',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs, 1)\n",
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational Inference: E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  The output of this function are the matricies gamma and phi, where gamma (k vector) is the Dirichlet paramteters and the matrix phi (N x k, where k is the number of topics) are the multinomial parameters.  See page 1004 of paper for derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    newPhi = oldPhi\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,k))\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*np.exp(scipy.special.psi(gamma[i]) - \n",
    "                                                                                       scipy.special.psi(np.sum(gamma)))\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        for i in range(0,k):\n",
    "            gamma[i] = alpha[i] + np.sum(newPhi[:, i]) #updating gamma\n",
    "\n",
    "\n",
    "        criteria = (1/(N*k)*np.sum((newPhi - oldPhi)**2))**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "np.random.seed(7)\n",
    "alphaOld = 10*np.random.rand(k)\n",
    "V = corpusMatrix[0][0].shape[1]\n",
    "betaOld = np.random.rand(k, V)\n",
    "betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 632 µs, sys: 11 µs, total: 643 µs\n",
      "Wall time: 650 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "phi, gamma = Estep(k, 0, alphaOld, betaOld, corpusMatrix, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 for derivation.  \n",
    "\n",
    "The alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the Mstep() function maximizes for alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "def alphaUpdate(k, M, alphaOld, gamma, tol):\n",
    "    h = np.zeros(k)\n",
    "    g = np.zeros(k)\n",
    "    alphaNew = np.zeros(k)\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(0, M):\n",
    "                docSum += scipy.special.psi(gamma[d][i]) - scipy.special.psi(np.sum(gamma[d]))\n",
    "            g[i] = M*(scipy.special.psi(sum(alphaOld)) - scipy.special.psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*scipy.special.polygamma(1, alphaOld[i])\n",
    "        z =  -scipy.special.polygamma(1, np.sum(alphaOld))\n",
    "        c = np.sum(g/h)/(1/z + np.sum(1/h))\n",
    "        step = (g - c)/h\n",
    "        alphaNew = alphaOld +step\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol):\n",
    "    #Calculate beta#\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(0,M):\n",
    "                Nd = corpusMatrix[0][d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi[d][n,i]*corpusMatrix[0][d][n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    ##Update ALPHA##\n",
    "    alphaNew = alphaUpdate(k, M, alphaOld, gamma, tol)\n",
    "    return(alphaNew, beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Function:**\n",
    "Finally, we implement the entire Latent Dirichlet Allocation method in the LDA function, which takes as its arguments k (the number of topics), D (the number of documents in the corpus), a corpus matrix (the output from make_word_matrix above) and a tolerance (which sets the convergence criteria for the while loops).  For each document d, the function runs until the alpha or beta parameters converge, by first running the E step and then the M step for each document separately.  The final values of phi, gamma, alpha and beta are returned for all D documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, corpusMatrix, tol):\n",
    "\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 1:\n",
    "        print(\"Number of topics must be a positive integer greater than 1\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    #alphaOld = 10*np.random.rand(k)\n",
    "    alphaOld = np.full(shape = k, fill_value = 50/k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        alphaNew, betaNew = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.13 ms, sys: 105 µs, total: 4.23 ms\n",
      "Wall time: 4.19 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[array([[ 0.31947491,  0.60375764,  0.07676745],\n",
       "          [ 0.21708246,  0.42940101,  0.35351653],\n",
       "          [ 0.36504825,  0.32103149,  0.31392025]]),\n",
       "   array([[ 0.22542555,  0.42560818,  0.34896627],\n",
       "          [ 0.37638558,  0.31593579,  0.30767864],\n",
       "          [ 0.48922886,  0.33497733,  0.17579382],\n",
       "          [ 0.48922886,  0.33497733,  0.17579382],\n",
       "          [ 0.48922886,  0.33497733,  0.17579382],\n",
       "          [ 0.48922886,  0.33497733,  0.17579382]]),\n",
       "   array([[ 0.37163282,  0.21033416,  0.41803302],\n",
       "          [ 0.27462986,  0.43546204,  0.2899081 ],\n",
       "          [ 0.41500911,  0.13653667,  0.44845421]]),\n",
       "   array([[ 0.26727293,  0.4345588 ,  0.29816827],\n",
       "          [ 0.08156863,  0.23734709,  0.68108429],\n",
       "          [ 0.16984398,  0.18240287,  0.64775315],\n",
       "          [ 0.29433738,  0.39535688,  0.31030574],\n",
       "          [ 0.4033363 ,  0.13606612,  0.46059757]])],\n",
       "  [array([ 17.74019471,  17.98671284,  17.44764112]),\n",
       "   array([ 19.39731565,  18.71397595,  18.06325706]),\n",
       "   array([ 17.89986089,  17.41485555,  17.85983221]),\n",
       "   array([ 18.05494833,  18.01825444,  19.10134589])],\n",
       "  array([ 16.87289586,  16.65740874,  16.73135473]),\n",
       "  array([[ 0.07711935,  0.09444166,  0.1426195 ,  0.12921551,  0.34104705,\n",
       "           0.06476738,  0.02960005,  0.01421561,  0.05567741,  0.05129649],\n",
       "         [ 0.15257918,  0.15525806,  0.04864686,  0.11366889,  0.23911119,\n",
       "           0.03753482,  0.03255039,  0.04235536,  0.10774252,  0.07055273],\n",
       "         [ 0.12415023,  0.10393111,  0.16065729,  0.10985556,  0.12427261,\n",
       "           0.07387924,  0.11447782,  0.12036845,  0.01356716,  0.05484053]])]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "LDA(3, corpusMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Most probable words**\n",
    "\n",
    "The following function takes $\\beta$, the list of words for a corpus, and the number of words to return for each topic, p, as its inputs. It returns the p words with the most probability for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to return the most probable words\n",
    "#p is the number of words you want returned for each topic\n",
    "def mostCommon(beta, wordList, p):\n",
    "    k = beta.shape[0]\n",
    "    topicWords = []\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    for i in range(0, k):\n",
    "        document = betaDF.loc[i,:]\n",
    "        document.sort(1, ascending = 0)\n",
    "        mostCommon = pd.DataFrame(document[0:p])\n",
    "        topicWords.append(mostCommon)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Profiling - State of the Union Example**\n",
    "\n",
    "In order to determine the initial performance time of our algorithm, we implemented LDA on a small corpus of eight documents. These documents are each of the State of the Union Adresses delivered by President Clinton during his time in office. These were found in the nltk Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"state_union\")\n",
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a diciontary of the state of the Union Addresses for Clinton\n",
    "fileNames = state_union.fileids()\n",
    "Clinton = fileNames[50:58]\n",
    "\n",
    "ClintonSOTU = {}\n",
    "for i in range(0, len(Clinton)):\n",
    "    ClintonSOTU[Clinton[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(Clinton[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileNames = state_union.fileids()\n",
    "SOTU = fileNames\n",
    "\n",
    "PresSOTU = {}\n",
    "for i in range(0, len(SOTU)):\n",
    "    PresSOTU[SOTU[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(SOTU[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "np.random.seed(17)\n",
    "PresCorp = make_word_matrix(PresSOTU, 0)\n",
    "ResultsAll = LDA(5, PresCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 53s, sys: 839 ms, total: 4min 54s\n",
      "Wall time: 4min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(7)\n",
    "ClintCorp = make_word_matrix(ClintonSOTU, 0)\n",
    "Results = LDA(3, ClintCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annayanchenko/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[                 0\n",
       " year      0.027425\n",
       " peopl     0.018757\n",
       " will      0.015316\n",
       " can       0.014768\n",
       " us        0.011999\n",
       " congress  0.010766\n",
       " govern    0.010200\n",
       " american  0.009560\n",
       " let       0.008880\n",
       " give      0.008825,                  1\n",
       " work      0.028984\n",
       " peopl     0.018815\n",
       " s         0.017864\n",
       " american  0.016418\n",
       " can       0.012685\n",
       " america   0.011470\n",
       " will      0.010532\n",
       " t         0.010124\n",
       " time      0.009811\n",
       " tax       0.009292,                 2\n",
       " must     0.015145\n",
       " s        0.013888\n",
       " will     0.011184\n",
       " world    0.010891\n",
       " way      0.009042\n",
       " help     0.008930\n",
       " deficit  0.008924\n",
       " famili   0.008755\n",
       " need     0.008169\n",
       " say      0.007506]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommon(Results[0][3], ClintCorp[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): memory-profiler in /Users/annayanchenko/anaconda3/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "#pip install --pre line-profiler\n",
    "#pip install psutil\n",
    "# pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file 'ClintonWordMatrix.prof'. \n",
      " \n",
      "*** Profile stats marshalled to file 'ClintonLDA.prof'. \n"
     ]
    }
   ],
   "source": [
    "#creating profile files for the construction of the word matrix and LDA for SOTU example\n",
    "%prun -q -D ClintonWordMatrix.prof make_word_matrix(ClintonSOTU,0)\n",
    "%prun -q -D ClintonLDA.prof LDA(3, ClintCorp, 0.1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 25 13:48:55 2016    ClintonWordMatrix.prof\n",
      "\n",
      "         1089458 function calls (1089034 primitive calls) in 1.095 seconds\n",
      "\n",
      "   Random listing order was used\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    30666    0.005    0.000    0.008    0.000 <ipython-input-10-d69bc967fb63>:96(<lambda>)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "     4795    0.001    0.000    0.001    0.000 <ipython-input-10-d69bc967fb63>:70(<genexpr>)\n",
      "    30210    0.030    0.000    0.058    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:470(_step4)\n",
      "        4    0.006    0.001    0.217    0.054 <ipython-input-10-d69bc967fb63>:102(<listcomp>)\n",
      "        4    0.007    0.002    0.240    0.060 <ipython-input-10-d69bc967fb63>:61(<listcomp>)\n",
      "    30210    0.037    0.000    0.073    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:252(_step1ab)\n",
      "        1    0.000    0.000    1.095    1.095 {built-in method builtins.exec}\n",
      "103826/103488    0.027    0.000    0.027    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:164(_cons)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:120(__init__)\n",
      "    16007    0.366    0.000    0.366    0.000 {method 'index' of 'list' objects}\n",
      "     2506    0.003    0.000    0.005    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:214(_vowelinstem)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
      "     5110    0.005    0.000    0.009    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:229(_cvc)\n",
      "   125360    0.014    0.000    0.014    0.000 {method 'lower' of 'str' objects}\n",
      "    30210    0.019    0.000    0.056    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:555(_step5)\n",
      "    32014    0.076    0.000    0.334    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:568(stem_word)\n",
      "        4    0.060    0.015    0.060    0.015 <ipython-input-10-d69bc967fb63>:100(<listcomp>)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.zeros}\n",
      "   189114    0.013    0.000    0.013    0.000 {built-in method builtins.len}\n",
      "     2250    0.001    0.000    0.001    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:221(_doublec)\n",
      "    30210    0.008    0.000    0.009    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:319(_step1c)\n",
      "    30210    0.014    0.000    0.019    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:435(_step3)\n",
      "30296/30210    0.027    0.000    0.040    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:343(_step2)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.117    0.117    1.086    1.086 <ipython-input-10-d69bc967fb63>:10(make_word_matrix)\n",
      "        1    0.000    0.000    0.000    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/stop_words/__init__.py:28(get_stop_words)\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method builtins.sorted}\n",
      "    16598    0.034    0.000    0.055    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:175(_m)\n",
      "    30666    0.007    0.000    0.010    0.000 <ipython-input-10-d69bc967fb63>:55(<lambda>)\n",
      "        4    0.066    0.016    0.066    0.016 <ipython-input-10-d69bc967fb63>:59(<listcomp>)\n",
      "   285132    0.044    0.000    0.044    0.000 {method 'endswith' of 'str' objects}\n",
      "    32014    0.062    0.000    0.068    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:598(_adjust_case)\n",
      "    32014    0.037    0.000    0.445    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/nltk/stem/porter.py:631(stem)\n",
      "        1    0.010    0.010    1.095    1.095 <string>:1(<module>)\n",
      "\n",
      "\n",
      "Mon Apr 25 13:50:24 2016    ClintonLDA.prof\n",
      "\n",
      "         420104 function calls in 88.676 seconds\n",
      "\n",
      "   Random listing order was used\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    51718    0.008    0.000    0.008    0.000 {built-in method builtins.isinstance}\n",
      "     3447    0.012    0.000    0.029    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py:1976(norm)\n",
      "    31007    0.019    0.000    0.050    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/numpy/core/numeric.py:406(asarray)\n",
      "        1    0.000    0.000   88.676   88.676 {built-in method builtins.exec}\n",
      "    13780    0.157    0.000    0.233    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/scipy/special/basic.py:551(polygamma)\n",
      "        4    0.000    0.000    0.000    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/numpy/core/numeric.py:247(full)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "    10335    0.023    0.000    0.023    0.000 {built-in method builtins.sum}\n",
      "    51717    0.015    0.000    0.111    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:31(_sum)\n",
      "        4   10.108    2.527   26.484    6.621 <ipython-input-13-e28af0607c9e>:1(Estep)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.copyto}\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.zeros}\n",
      "        1   61.507   61.507   62.191   62.191 <ipython-input-15-c1993c175e2b>:1(Mstep)\n",
      "    51717    0.096    0.000    0.096    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.001    0.001   88.676   88.676 <ipython-input-16-4af8bf51643c>:3(LDA)\n",
      "     3447    0.001    0.000    0.002    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py:111(isComplexType)\n",
      "    96042   16.374    0.000   16.374    0.000 {method 'index' of 'list' objects}\n",
      "     3447    0.001    0.000    0.001    0.000 {built-in method builtins.issubclass}\n",
      "    13780    0.029    0.000    0.029    0.000 {built-in method numpy.core.multiarray.where}\n",
      "     3447    0.003    0.000    0.003    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "     3447    0.009    0.000    0.009    0.000 {built-in method numpy.core.multiarray.dot}\n",
      "    31011    0.032    0.000    0.032    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'rand' of 'mtrand.RandomState' objects}\n",
      "        1    0.235    0.235    0.684    0.684 <ipython-input-14-c98e4e5a6979>:2(alphaUpdate)\n",
      "        1    0.000    0.000   88.676   88.676 <string>:1(<module>)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
      "    51717    0.046    0.000    0.165    0.000 /Users/annayanchenko/anaconda3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:1733(sum)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing profile results\n",
    "import pstats\n",
    "wordMatrixProfile = pstats.Stats('ClintonWordMatrix.prof')\n",
    "LDAProfile = pstats.Stats('ClintonLDA.prof')\n",
    "wordMatrixProfile.print_stats()\n",
    "LDAProfile.print_stats()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cythonizing**\n",
    "\n",
    "The following code chunks are the Cython versions of the code for updating $\\alpha$ and $\\beta$. These can be executed in order to examine the areas where the function still heavily relies on Python. We would like to thank Professor Chan for his assistance with the Cython portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython -a -lgsl\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import scipy\n",
    "from cython_gsl cimport *\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:] cy_alphaUpdate(int k, int M, double[:] alphaOld, double[:, :] gamma, double tol):\n",
    "    cdef double[:] h = np.zeros(k)\n",
    "    cdef double[:] g = np.zeros(k)\n",
    "    cdef double[:] alphaNew = np.zeros(k)\n",
    "\n",
    "    cdef int converge\n",
    "    cdef int i, d\n",
    "    cdef double docSum\n",
    "    cdef double[:] step = np.zeros(k)\n",
    "    cdef double c, s1, s2\n",
    "\n",
    "    cdef double alpha_sum = 0\n",
    "    for i in range(k):\n",
    "        alpha_sum += alphaOld[i]\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        z = -gsl_sf_psi_n(1, alpha_sum)\n",
    "\n",
    "        s1 = 0\n",
    "        s2 = 1.0/z\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(M):\n",
    "                docSum += gsl_sf_psi(gamma[d,i]) - gsl_sf_psi(sum(gamma[d]))\n",
    "            g[i] = M*(gsl_sf_psi(alpha_sum) - gsl_sf_psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*gsl_sf_psi_n(1, alphaOld[i])\n",
    "            \n",
    "            s1 += g[i]/h[i]\n",
    "            s2 += 1.0/h[i]\n",
    "        c = s1/s2\n",
    "\n",
    "        for i in range(k):\n",
    "            step[i] = (g[i] - c)/h[i]\n",
    "        # step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            for i in range(k):\n",
    "                alphaNew[i] = alphaOld[i] + step[i]\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:, :] cy_betaUpdate(int k, int M, long[:] phi_1, double[:, :, :] phi_2, double[:, :] gamma, \n",
    "                              long[:] c_1, double[:, :, :] c_2, double tol):\n",
    "\n",
    "    cdef int i, j, d, n\n",
    "    cdef int Nd\n",
    "    cdef double wordSum\n",
    "\n",
    "    #Calculate beta#\n",
    "    cdef int V = c_2[0].shape[1]\n",
    "    cdef double[:, :] beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(M):\n",
    "                Nd = c_1[d] # c[d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi_2[d, n,i]*c_2[d, n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed-Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit a, b = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "betaNew = np.array(cy_betaUpdate(k, M, phi_1, phi_2, gamma, c_1, c_2, tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, corpusMatrix, tol):\n",
    "\n",
    "    # create rectangular matrices for c\n",
    "    c = corpusMatrix[0]\n",
    "    r_c = len(c)\n",
    "    c_1 = np.array([c_.shape[0] for c_ in c]).astype('int')\n",
    "    n_c = max(c_1)\n",
    "    p_c = c[0].shape[1]\n",
    "    c_2 = np.zeros((r_c, n_c, p_c))\n",
    "    for i, j in enumerate(c_1):\n",
    "        c_2[i, :j, :] = c[i]\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = 10*np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        # create rectangular matrices for phi\n",
    "        r = len(phi)\n",
    "        phi_1 = np.array([p_.shape[0] for p_ in phi]).astype('int')\n",
    "        n = max(phi_1)\n",
    "        p = phi[0].shape[1]\n",
    "        phi_2 = np.zeros((r, n, p))\n",
    "        for i, j in enumerate(phi_1):\n",
    "            phi_2[i, :j, :] = phi[i]\n",
    "\n",
    "                \n",
    "        gamma = np.array(gamma)\n",
    "\n",
    "        alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "        betaNew = np.array(cy_betaUpdate(k, M, phi_1, phi_2, gamma, c_1, c_2, tol))\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LDA(3, corpusMatrix, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
