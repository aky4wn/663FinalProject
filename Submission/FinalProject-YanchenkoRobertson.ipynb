{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Abstract**\n",
    "\n",
    "Latent Dirichlet Analysis (LDA) is a method used to model the generative process of creating discrete data such as text corpora. LDA can take a large amount of data and create descriptions of that data. This method reduces the size of the data to these short descriptions while still maintaining the relationships necessary to carry out various inferences.  For this project, we implemented the LDA algorithm presented in the paper, “Latent Dirichlet Allocation”, written by David M. Blei, Andrew Y. Ng, and Michael I. Jordan. The implementation of the algorithm is used to analyze text of famous American historical documents. In addition, it is used to create clusters of movies based on user rating data. \n",
    "\n",
    "**II. Background**\n",
    "\n",
    "The algorithm assumes a generative process for the creation of documents in a corpus D using N words. The topic for a particular word, $z_n$, is modeled as Multinomial($\\theta$), where $\\theta \\sim Dir(\\alpha)$. Then, each word, $w_n$, is chosen from a multinomial probability that is conditioned on the topic $z_n$, P($w_n | z_n, \\beta)$.  (Blei et al., pg. 996)\n",
    "\n",
    "LDA is most well-known for its application in the analysis of text data. It is used to create topics for documents, classify documents based on these topics and to determine which documents are similar to one another. The algorithm can also be used in other problems that have a similar structure to the document generating method described above. For example, in Blei et.al, 2003, the authors used a data set where web site users provide information about movies they enjoy. In this example, the users are analogous to the documents and their preferred movies are “words”.  Topics can be found by determining similar movies. \n",
    "\n",
    "There are multiple alternative algorithms that can be used for text analysis. These include the term-frequency inverse-document-frequency matrix, latent semantic indexing (LSI) and the probabilistic latent semantic indexing (pLSI) model. In Section 7 of the paper, the authors present the results of document modeling and document classification. In this section, the authors’ results show that their implementation of LDA performs better than competing methods in terms of perplexity measure, where better generalization performance is defined by a smaller perplexity measure. \n",
    "\n",
    " In addition, this section demonstrates that other methods are prone to overfitting. As the number of topic increases, some of the alternative algorithms induce words that have small probabilities. This occurs because the documents in the corpus are divided into more collections. This can result in the perplexity measure becoming very large for these alternative algorithms. The problem comes from the requirement that the topic proportions in a future document must be seen in at least one of the training documents. On the other hand, LDA does not have this overfitting problem. For more detail, see Section 7 of Blei et al.\n",
    "\n",
    "A disadvantage of LDA is that exact inference of the posterior is impossible as a result of intractability. Thus, it is necessary for the user to implement an approximating technique such as variational inference, a Gibbs sampler, or another technique. The approximation of the posterior can be computationally intensive and also time-consuming. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III. Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Code**\n",
    "\n",
    "The LDA algorithm was implemented using the Python programming language in the Jupyter notebook environment. The LDA function requires four arguments. The number of topics, k, must be specified. In addition, the output from the make_word_matrix is necessary in the LDA function. This function takes the corpus as an input and returns three things: a matrix where each word in the document is a row and the columns are the unique words in the corpus, the list of words unique to the corpus, and the number of documents in the corpus. The third argument necessary for the LDA function is the tolerance for convergence. Finally, an indicator value of the form of the corpus documents is required for the LDA. This value is 0 if the documents are a list of strings, and the value is 1 if the documents are just one long string. In addition, we created a function to return a specified number of words for each topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.\tTesting and Base Cases**\n",
    "\n",
    "In addition to the implementation described above, various checks have been included in the function to prevent incorrect arguments.  One check is that the number of topics specified by the user must be greater than one. It is not possible to have zero or less than zero topics, and one topic would just be described by the entire document. In addition, there is a check in place to ensure that the corpus is not empty and that each element of the corpus is a string. This prevents the user from receiving an error that the input is not a string. Lastly, there are checks to ensure that the tolerance is greater than zero and that the entry for needToSplit is either zero or one. These checks print messages that inform the user of why their input was problematic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in /Users/annayanchenko/anaconda3/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): nltk in /Users/annayanchenko/anaconda3/lib/python3.5/site-packages\n"
     ]
    }
   ],
   "source": [
    "! pip install stop_words\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy Example Corpus **\n",
    "From HW5 Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix - make_word_matrix()**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "    \n",
    "    \n",
    "    #Check to make sure NeedToSplit argument is 0 or 1\n",
    "    if needToSplit != 0 and needToSplit != 1:\n",
    "        print(\"NeedToSplit argument must be 0 or 1\")\n",
    "        return;\n",
    "    \n",
    "    #Define stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    \n",
    "    #Initialize stemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    \n",
    "    #Check to make sure that dictionary isn't empty#\n",
    "    if M ==0:\n",
    "        print(\"Input dictionary is empty\")\n",
    "        return;\n",
    "    \n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        if isinstance(corpus[i], str) != True:\n",
    "            print(\"Corpus input is not a string\")\n",
    "            return;\n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        # stem tokens\n",
    "        text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "            #Remove stop words#\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "            \n",
    "            #remove stop words\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = wordOrder.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'eleph',\n",
       "  'fox',\n",
       "  'jump',\n",
       "  'lazi',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs, 1)\n",
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  The output of this function are the matricies gamma and phi, where gamma (k vector) is the Dirichlet paramteters and the matrix phi (N x k, where k is the number of topics) are the multinomial paramters.  See page 1004 of paper for derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    newPhi = oldPhi\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,k))\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*np.exp(scipy.special.psi(gamma[i]))\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        for i in range(0,k):\n",
    "            gamma[i] = alpha[i] + np.sum(newPhi[:, i]) #updating gamma\n",
    "\n",
    "\n",
    "        criteria = (1/(N*k)*np.sum((newPhi - oldPhi)**2))**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parameter Estimation**\n",
    "\n",
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 for derivation.  \n",
    "\n",
    "The alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the Mstep() function maximizes for alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython  -lgsl\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import scipy\n",
    "from cython_gsl cimport *\n",
    "\n",
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:] cy_alphaUpdate(int k, int M, double[:] alphaOld, double[:, :] gamma, double tol):\n",
    "    cdef double[:] h = np.zeros(k)\n",
    "    cdef double[:] g = np.zeros(k)\n",
    "    cdef double[:] alphaNew = np.zeros(k)\n",
    "\n",
    "    cdef int converge\n",
    "    cdef int i, d\n",
    "    cdef double docSum\n",
    "    cdef double[:] step = np.zeros(k)\n",
    "    cdef double c, s1, s2\n",
    "\n",
    "    cdef double alpha_sum = 0\n",
    "    for i in range(k):\n",
    "        alpha_sum += alphaOld[i]\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        z = -gsl_sf_psi_n(1, alpha_sum)\n",
    "\n",
    "        s1 = 0\n",
    "        s2 = 1.0/z\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(M):\n",
    "                docSum += gsl_sf_psi(gamma[d,i]) - gsl_sf_psi(sum(gamma[d]))\n",
    "            g[i] = M*(gsl_sf_psi(alpha_sum) - gsl_sf_psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*gsl_sf_psi_n(1, alphaOld[i])\n",
    "            \n",
    "            s1 += g[i]/h[i]\n",
    "            s2 += 1.0/h[i]\n",
    "        c = s1/s2\n",
    "\n",
    "        for i in range(k):\n",
    "            step[i] = (g[i] - c)/h[i]\n",
    "        # step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            for i in range(k):\n",
    "                alphaNew[i] = alphaOld[i] + step[i]\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:, :] cy_betaUpdate(int k, int M, long[:] phi_1, double[:, :, :] phi_2, double[:, :] gamma, \n",
    "                              long[:] c_1, double[:, :, :] c_2, double tol):\n",
    "\n",
    "    cdef int i, j, d, n\n",
    "    cdef int Nd\n",
    "    cdef double wordSum\n",
    "\n",
    "    #Calculate beta#\n",
    "    cdef int V = c_2[0].shape[1]\n",
    "    cdef double[:, :] beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(M):\n",
    "                Nd = c_1[d] # c[d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi_2[d, n,i]*c_2[d, n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Function:**\n",
    "Finally, we implement the entire Latent Dirichlet Allocation method in the LDA function, which takes as its arguments k (the number of topics),  a corpus matrix (the output from make_word_matrix above) and a tolerance (which sets the convergence criteria for the while loops).  For each document d, the function runs the E step, then for all documents runs the M step update for alpha and beta.  This variational update than parameter estimation update continues until alpha or beta converges to within the specified tolerance.  The final values of phi, gamma, alpha and beta are returned for all D documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, corpusMatrix, tol):\n",
    "\n",
    "    # create rectangular matrices for c\n",
    "    c = corpusMatrix[0]\n",
    "    r_c = len(c)\n",
    "    c_1 = np.array([c_.shape[0] for c_ in c]).astype('int')\n",
    "    n_c = max(c_1)\n",
    "    p_c = c[0].shape[1]\n",
    "    c_2 = np.zeros((r_c, n_c, p_c))\n",
    "    for i, j in enumerate(c_1):\n",
    "        c_2[i, :j, :] = c[i]\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = np.full(shape = k, fill_value = 50/k) + np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        # create rectangular matrices for phi\n",
    "        r = len(phi)\n",
    "        phi_1 = np.array([p_.shape[0] for p_ in phi]).astype('int')\n",
    "        n = max(phi_1)\n",
    "        p = phi[0].shape[1]\n",
    "        phi_2 = np.zeros((r, n, p))\n",
    "        for i, j in enumerate(phi_1):\n",
    "            phi_2[i, :j, :] = phi[i]\n",
    "\n",
    "                \n",
    "        gamma = np.array(gamma)\n",
    "        alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "        betaNew = np.array(cy_betaUpdate(k, M, phi_1, phi_2, gamma, c_1, c_2, tol))\n",
    "    \n",
    "        #if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "        if np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 ms, sys: 1.74 ms, total: 5.4 ms\n",
      "Wall time: 9.11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(97)\n",
    "res = LDA(3, corpusMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[array([[ 0.09689023,  0.5125238 ,  0.39058597],\n",
       "          [ 0.10309406,  0.63093592,  0.26597002],\n",
       "          [ 0.5518897 ,  0.17445205,  0.27365826]]),\n",
       "   array([[ 0.11610625,  0.62060252,  0.26329123],\n",
       "          [ 0.58413674,  0.16126668,  0.25459658],\n",
       "          [ 0.72925797,  0.13686583,  0.1338762 ],\n",
       "          [ 0.72925797,  0.13686583,  0.1338762 ],\n",
       "          [ 0.72925797,  0.13686583,  0.1338762 ],\n",
       "          [ 0.72925797,  0.13686583,  0.1338762 ]]),\n",
       "   array([[ 0.16226174,  0.6583813 ,  0.17935696],\n",
       "          [ 0.40071343,  0.14074978,  0.45853679],\n",
       "          [ 0.17916578,  0.23317472,  0.5876595 ]]),\n",
       "   array([[ 0.3948353 ,  0.14007042,  0.46509427],\n",
       "          [ 0.1557428 ,  0.20170723,  0.64254998],\n",
       "          [ 0.24561568,  0.43189135,  0.32249297],\n",
       "          [ 0.17846235,  0.63008699,  0.19145066],\n",
       "          [ 0.17572041,  0.23097513,  0.59330447]])],\n",
       "  array([[ 18.63161569,  18.71214015,  17.96387036],\n",
       "         [ 21.49701656,  18.72356092,  18.08704871],\n",
       "         [ 18.62188265,  18.42653418,  18.25920936],\n",
       "         [ 19.03011823,  19.0289595 ,  19.24854846]]),\n",
       "  array([ 17.98476059,  17.34928734,  17.03592818]),\n",
       "  array([[ 0.0350067 ,  0.12705064,  0.056676  ,  0.18142558,  0.46585553,\n",
       "           0.02591351,  0.03922529,  0.02487242,  0.01547355,  0.02850078],\n",
       "         [ 0.23550474,  0.05284256,  0.0873401 ,  0.06317293,  0.10301738,\n",
       "           0.12388906,  0.08126995,  0.03795569,  0.09644273,  0.11856486],\n",
       "         [ 0.09757672,  0.17028432,  0.21772724,  0.09739117,  0.0987278 ,\n",
       "           0.03306697,  0.05945609,  0.11846308,  0.07200999,  0.03529661]])]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Find Highest Probability Words for Each Topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to return the most probable words\n",
    "#p is the number of words you want returned for each topic\n",
    "def mostCommon(beta, wordList, p):\n",
    "    k = beta.shape[0]\n",
    "    topicWords = []\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    for i in range(0, k):\n",
    "        document = betaDF.loc[i,:]\n",
    "        document.sort(1, ascending = 0)\n",
    "        mostCommon = pd.DataFrame(document[0:p])\n",
    "        topicWords.append(mostCommon)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.\tTests for Correctness**\n",
    "\n",
    "*i.\tGenerative Model*\n",
    "\n",
    "A generative model check was utilized in order to evaluate the functionality of the algorithm. A random number was generated in order to determine the number of documents from a Poisson(5) distribution, and the resulting number was six. Then, for each of the six documents, a random number of words for each document was drawn from a Poisson(5) distribution. These documents had between 2-6 words. Random values were used to create a $\\beta$ vector, and the $\\alpha$ vector was set to be 50/k, where k is the number of topics and k = 3. The 50/k value was chosen based on the results shown in Gregor Heinrich’s “Parameter estimation for text analysis”.  (Heinrich, 2008, 24). The corpus was defined to have five unique words that are not stop words.\n",
    "The set-up described in the previous paragraph was used to determine which of the words were in each of the documents. This process followed the generative model described in Blei et. al’s paper. (Blei et al, 2003, 996). Once the documents were generated, our LDA implementation was used on this corpus to estimate the $\\alpha$ and $\\beta$ values.\n",
    "\n",
    "\\begin{center}\n",
    " \\begin{tabular}{||c c||} \n",
    " \\hline\n",
    " Alpha & Beta \\\\ [0.5ex] \n",
    " \\hline\n",
    " 0.06 & 0.63\\\\ [1ex] \n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "Thus, the values returned by the algorithm are relatively close to the ones used to generate the corpus. It is not surprising that the mean squared errors for the $\\beta$ vector is larger because there are more elements in the beta vector. The $\\alpha$ vector converges faster than the $\\beta$ vector, so this could be the reason for the larger MSE for $\\beta$. (EXPAND HERE?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.,  3.,  1.,  3.,  1.]),\n",
       " array([ 4.,  3.,  1.,  2.]),\n",
       " array([ 1.,  4.,  1.]),\n",
       " array([ 1.,  0.,  3.]),\n",
       " array([ 3.,  1.,  1.,  0.,  4.]),\n",
       " array([ 1.,  4.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate data\n",
    "np.random.seed(7)\n",
    "#Draw number of documents\n",
    "M = np.random.poisson(lam = 5)\n",
    "k = 3\n",
    "alpha = np.full(shape = k, fill_value = 50/k)\n",
    "V = 5\n",
    "betaOld = np.random.rand(k, V)\n",
    "betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "corpus = []\n",
    "for i in range(0,M):\n",
    "    N = np.random.poisson(5)\n",
    "    theta = np.random.dirichlet(alpha)\n",
    "    docWords = np.zeros(N)\n",
    "    for j in range(0,N):\n",
    "        z = np.random.multinomial(1,theta)\n",
    "        zn = list(z).index(1)\n",
    "        w = np.random.multinomial(1,betaOld[zn,:])\n",
    "        docWords[j] = list(w).index(1)\n",
    "    corpus.append(docWords)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word choices apple, banana, grape, orange, watermelon\n",
    "s1 = \"Watermelon orange banana orange banana\"\n",
    "s2 = \"Watermelon orange banana grape\"\n",
    "s3 = \"Banana watermelon banana.\"\n",
    "s4 = \"Banana apple orange\"\n",
    "s5 = \"Orange banana banana apple watermelon\"\n",
    "s6 = \"Banana watermelon\"\n",
    "docsTest = collections.OrderedDict()\n",
    "docsTest[\"s1\"] = s1\n",
    "docsTest[\"s2\"] = s2\n",
    "docsTest[\"s3\"] = s3\n",
    "docsTest[\"s4\"] = s4 \n",
    "docsTest[\"s5\"] = s5 \n",
    "docsTest[\"s6\"] = s6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16.66666667,  16.66666667,  16.66666667])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03101185,  0.11554004,  0.21515669,  0.29235046,  0.34594096],\n",
       "       [ 0.2050273 ,  0.03548777,  0.15508358,  0.48955467,  0.11484668],\n",
       "       [ 0.15280097,  0.31471277,  0.00841501,  0.20296305,  0.3211082 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betaOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "corpusMatTest = make_word_matrix(docsTest, 1)\n",
    "phi, gamma, alphaNew, betaNew =  LDA(3, corpusMatTest, 0.1)[0]\n",
    "alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26485668,  0.5609513 ,  0.01196623,  0.1407786 ,  0.02144719],\n",
       "       [ 0.1026411 ,  0.40340518,  0.03093386,  0.28825866,  0.17476119],\n",
       "       [ 0.03782621,  0.39367916,  0.07948585,  0.12391   ,  0.36509877]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betaNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alpha MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(alpha - alphaNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beta MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(betaOld - betaNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ii.\tComparison to Python package*\n",
    "\n",
    "In addition to the generative model, the performance of our LDA implementation was compared to the implementation from the genism package. This implementation can be found at https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html. \n",
    "\n",
    "**need to add chart for comparison, set a seed?**\n",
    "\n",
    "The results from the Python package and our LDA algorithm were not exactly the same. There is one topic that is very similar for both of the algorithms. These topics have many of the same wrods and some of the probabilities assigned to each of these words. On the other hand, there are also some topics that are not very similar. This could be a result of some of the differences between the two algorithms. The Python implmentation requires the user to pass the number of iterations to run as an argument whereas our implemnentation sets convergence criterion. In addition, the Python implmentation randomly initiates all of the parameters. Thus, the differences between the starting positions and the convergence criterion could result in different topics. Therefore, we are satisfied with the similarity found between our LDA implementation and that using the gensim package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"The quick brown fox\"\n",
    "doc_b = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "doc_c =  \"The the the lazy dog elephant.\"\n",
    "doc_d = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, iterations = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Package Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our LDA Function Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostCommon(res[0][3], corpusMatrix[1], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.\tProfiling Code and Speed Up**\n",
    "\n",
    "We profiled the initial code by implementing the algorithm on a corpus of four of President Clinton’s State of the Union Addresses. It was found that the main bottleneck in the algorithm was the maximization step of the algorithm. About 75% of the time of the LDA algorithm was spent on the maximization step. In the maximization step, the $\\alpha$ update occurs in a while loop, and the $\\beta$ update uses four for loops. The expectation step also slowed down the code, but only accounted for approximately 13% of the time of the LDA implementation. This mostly likely is a result of having fewer for loops than the maximization step. \n",
    "\n",
    "We identified inefficiencies in the matrix construction function and implemented a storage system in order to avoid the unnecessary repetition of some calculations. In addition, during the initial coding of the algorithm, care was taken to avoid for loops and use vectorization where possible. Just in time compilation (Numba) was also added to the code for the algorithm implementation. \n",
    "\n",
    "Finally, with the assistance of Professor Chan, the code was adapted so that Cython could be used. It was first necessary to adjust the code to prevent ragged arrays because Cython does not easily work with sparse or ragged arrays. The phi and the corpus matrix both initially used ragged arrays. To fix this, the largest dimension of the matrices was found, and the rest of the matrices were filled in with zeros such that the dimensions matched. In addition, the digamma function from the Cython GSL package replaced that from the scipy package. The final change was using cdef to define the inputs to the functions and the variables used within. \n",
    "\n",
    "\\begin{table}[H]\n",
    "\\caption{Toy Example Times} \\label{tab:title}\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c | l}\n",
    "  \\hline   \n",
    "  Method & Total Time \\\\\n",
    "  \\hline\n",
    "  No Speed-Up & 4.36 milliseconds \\\\\n",
    "  Numba & 3.02 seconds \\\\\n",
    "  Cython & 4.16 milliseconds \\\\\n",
    "  \\hline  \n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\\end{table}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\caption{Clinton SOTU Timing} \\label{tab:title}\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c | l}\n",
    "  \\hline   \n",
    "  Method & Total Time \\\\\n",
    "  \\hline\n",
    "  No Speed-Up & 4 mintues, 54 seconds \\\\\n",
    "  Numba & 4 minutes, 51 seconds  \\\\\n",
    "  Cython & 1 minute, 48 seconds \\\\\n",
    "  \\hline  \n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\\end{table}\n",
    "\n",
    "As the tables above show, the Cython provided the fastest implementation of the algorithm. For the smaller example, Numba performed the slowest and was only marginally faster in the State of the Union example. According to the Numba documentation, there is compilation time associated with Numba. In addition, it would be faster if Numba was used in No Python mode. However, this was not possible given the use of the numpy package. For the State of the Union example, the Cython code provides approximately a three time speed up over the other implementations.\n",
    "\n",
    "\n",
    "**See appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time LDA on 8 Clinton State of the Union Speeches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "#creating a diciontary of the state of the Union Addresses for Clinton\n",
    "fileNames = state_union.fileids()\n",
    "Clinton = fileNames[50:58]\n",
    "\n",
    "ClintonSOTU = {}\n",
    "for i in range(0, len(Clinton)):\n",
    "    ClintonSOTU[Clinton[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(Clinton[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(7)\n",
    "ClintCorp = make_word_matrix(ClintonSOTU, 0)\n",
    "Results = LDA(3, ClintCorp, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV.\tResults**\n",
    "\n",
    "The LDA algorithm was used to determine the topics that occurred in famous documents and speeches from American history. These included the Gettysburg Address, the Declaration of Independence, Martin Luther King’s “I Have A Dream” speech and President John F. Kennedy’s inauguration address. The first implementation defined the number of topics to be three. \n",
    "\n",
    "\\begin{table}[H]\n",
    "\\caption{American Document Topics - Three Topics} \\label{tab:title}\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c | l}\n",
    "  \\hline   \n",
    "  Topic & Potential Occurring Themes \\\\\n",
    "  \\hline\n",
    "  1 & will, can, one, live, men \\\\\n",
    "  2 & mentioning family members, ready for execution  \\\\\n",
    "  3 & not as clear as other topics, life, family, time \\\\\n",
    "  \\hline  \n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the topic assignments, the following excerpt from The Declaration of Independence. If a particular word was assigned to more than one topic, it was assigned to the topic for which it had the highest probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\">Topic 1</span> \n",
    "\n",
    "<span style=\"color:blue\">Topic 2</span> \n",
    "\n",
    "<span style=\"color:green\">Topic 3</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When in the <span style=\"color:blue\">Course</span>  of <span style=\"color:green\">human</span> <span style=\"color:red\">events</span> , it <span style=\"color:red\">becomes</span> <span style=\"color:green\">necessary</span> for <span style=\"color:green\">one</span> <span style=\"color:green\">people</span> to <span style=\"color:green\">dissolve</span> the <span style=\"color:green\">political</span> <span style=\"color:red\">bands</span> \n",
    "which have <span style=\"color:blue\">connected</span> them with <span style=\"color:green\">another</span>, and to <span style=\"color:blue\">assume</span>, <span style=\"color:blue\">among</span> the <span style=\"color:green\">Powers</span> of the <span style=\"color:green\">earth</span> , the <span style=\"color:green\">separate</span> and <span style=\"color:green\">equal</span> \n",
    "<span style=\"color:blue\">station</span> to which the <span style=\"color:blue\">Laws</span> of <span style=\"color:blue\">Nature</span> and of <span style=\"color:blue\">Nature's</span> <span style=\"color:green\">God</span> <span style=\"color:green\">entitle</span> them, a <span style=\"color:red\">decent</span> <span style=\"color:red\">respect</span> to the <span style=\"color:red\">opinions</span> of \n",
    "<span style=\"color:red\">mankind</span> <span style=\"color:green\">requires</span> that they should \n",
    "<span style=\"color:green\">declare</span> the <span style=\"color:red\">causes</span> which <span style=\"color:red\">impel</span> them to the <span style=\"color:green\">separation</span>.\n",
    "\n",
    "We <span style=\"color:blue\">hold</span> these <span style=\"color:green\">truths</span> to be <span style=\"color:blue\">self-evident</span>, that all <span style=\"color:blue\">men</span> are <span style=\"color:red\">created</span> <span style=\"color:green\">equal</span>, that they are <span style=\"color:blue\">endowed</span> by their \n",
    "<span style=\"color:green\">Creator</span> with <span style=\"color:blue\">certain unalienable Rights</span>, that <span style=\"color:blue\">among</span> these are <span style=\"color:blue\">Life</span>, <span style=\"color:green\">Liberty</span>, and the <span style=\"color:red\">pursuit</span> of <span style=\"color:blue\">Happiness</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second implementation defined the number of topics to be eight.\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\caption{American Document Topics - Eight Topics} \\label{tab:title}\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c | l}\n",
    "  \\hline   \n",
    "  Topic & Most Probable Words \\\\\n",
    "  \\hline\n",
    "  1 & will, can, one, live, men \\\\\n",
    "  2 & one, us, freedom, govern, power  \\\\\n",
    "  3 & one, us, everi, new, let, state \\\\\n",
    "  4 & will, world, new, freedom, today \\\\\n",
    "  5 & right, let, will, peopl, nation \\\\\n",
    "  6 & right, let, us, nation, one \\\\\n",
    "  7 & let, will, freedom, nation, everi\\\\\n",
    "  8 & let, state, freedom, us, power\\\\\n",
    "  \\hline  \n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\\end{table}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Topic 0</span> \n",
    "<span style=\"color:blue\">Topic 1</span> \n",
    "<span style=\"color:green\">Topic 2</span> \n",
    "<span style=\"color:darkmagenta\">Topic 3</span> \n",
    "<span style=\"color:orange\">Topic 4</span> \n",
    "<span style=\"color:lightseagreen\">Topic 5</span> \n",
    "<span style=\"color:darkblue\">Topic 6</span> \n",
    "<span style=\"color:magenta\">Topic 7</span> \n",
    "<span style=\"color:cyan\">Topic 8</span> \n",
    "<span style=\"color:chartreuse\">Topic 9</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declaration**\n",
    "\n",
    "When in the <span style=\"color:chartreuse\">Course</span> of <span style=\"color:lightseagreen\">human</span> <span style=\"color:red\">events</span>, it <span style=\"color:red\">becomes</span> <span style=\"color:green\">necessary</span> for <span style=\"color:blue\">one</span> <span style=\"color:orange\">people</span> to <span style=\"color:darkmagenta\">dissolve</span> the <span style=\"color:magenta\">political</span> <span style=\"color:cyan\">bands</span> \n",
    "which have <span style=\"color:darkblue\">connected</span> them with <span style=\"color:darkmagenta\">another</span>, and to <span style=\"color:darkmagenta\">assume</span>, <span style=\"color:blue\">among</span> the <span style=\"color:chartreuse\">Powers</span> of the <span style=\"color:chartreuse\">earth</span>, the <span style=\"color:chartreuse\">separate</span> and <span style=\"color:orange\">equal</span> \n",
    "<span style=\"color:lightseagreen\">station</span> to which the <span style=\"color:orange\">Laws</span> of <span style=\"color:lightseagreen\">Nature</span> and of <span style=\"color:lightseagreen\">Nature's</span> <span style=\"color:darkmagenta\">God</span> <span style=\"color:magenta\">entitle</span> them, a <span style=\"color:lightseagreen\">decent</span> <span style=\"color:green\">respect</span> to the <span style=\"color:darkmagenta\">opinions</span> of \n",
    "<span style=\"color:lightseagreen\">mankind</span> <span style=\"color:magenta\">requires</span> that they should \n",
    "<span style=\"color:lightseagreen\">declare</span> the <span style=\"color:orange\">causes</span> which <span style=\"color:lightseagreen\">impel</span> them to the <span style=\"color:chartreuse\">separation</span>.\n",
    "\n",
    "We <span style=\"color:magenta\">hold</span> these <span style=\"color:blue\">truths</span> to be <span style=\"color:chartreuse\">self-evident</span>, that all <span style=\"color:red\">men</span> are <span style=\"color:chartreuse\">created</span> <span style=\"color:orange\">equal</span>, that they are <span style=\"color:blue\">endowed</span> by their \n",
    "<span style=\"color:lightseagreen\">Creator</span> with <span style=\"color:orange\">certain</span> <span style=\"color:chartreuse\">unalienable</span> <span style=\"color:lightseagreen\">Rights</span>, that <span style=\"color:blue\">among</span> these are <span style=\"color:red\">Life</span>, <span style=\"color:darkblue\">Liberty</span>, and the <span style=\"color:cyan\">pursuit</span> of <span style=\"color:magenta\">Happiness</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gettysburg**\n",
    "\n",
    "It is for <span style=\"color:lightseagreen\">us</span> the <span style=\"color:red\">living</span>, <span style=\"color:orange\">rather</span>, to be <span style=\"color:orange\">dedicated</span> here to the <span style=\"color:orange\">unfinished</span>\n",
    "<span style=\"color:magenta\">work</span> which they who <span style=\"color:cyan\">fought</span> here have <span style=\"color:orange\">thus</span> <span style=\"color:darkblue\">far</span> so <span style=\"color:lightseagreen\">nobly</span> <span style=\"color:lightseagreen\">advanced</span>.\n",
    "It is <span style=\"color:orange\">rather</span> for <span style=\"color:lightseagreen\">us</span> to be here <span style=\"color:orange\">dedicated</span> to the <span style=\"color:red\">great</span> <span style=\"color:lightseagreen\">task</span> <span style=\"color:chartreuse\">remaining</span>\n",
    "before <span style=\"color:lightseagreen\">us</span>. . .that from these <span style=\"color:red\">honored</span> <span style=\"color:darkblue\">dead</span> we <span style=\"color:magenta\">take</span> <span style=\"color:orange\">increased</span> <span style=\"color:green\">devotion</span>\n",
    "to that <span style=\"color:orange\">cause</span> for which they <span style=\"color:orange\">gave</span> the <span style=\"color:orange\">last</span> <span style=\"color:blue\">full</span> <span style=\"color:magenta\">measure</span> of <span style=\"color:green\">devotion</span>. . .\n",
    "that we here <span style=\"color:darkblue\">highly</span> <span style=\"color:darkblue\">resolve</span> that these <span style=\"color:darkblue\">dead</span> <span style=\"color:magenta\">shall</span> not have <span style=\"color:darkblue\">died</span> in <span style=\"color:orange\">vain</span>. . .\n",
    "that this <span style=\"color:lightseagreen\">nation</span>, under <span style=\"color:darkmagenta\">God</span>, <span style=\"color:magenta\">shall</span> have a <span style=\"color:darkblue\">new</span> <span style=\"color:lightseagreen\">birth</span> of <span style=\"color:cyan\">freedom</span>. . .\n",
    "and that <span style=\"color:darkblue\">government</span> of the <span style=\"color:orange\">people</span>. . .by the <span style=\"color:orange\">people</span>. . .for the <span style=\"color:orange\">people</span>. . .\n",
    "<span style=\"color:magenta\">shall</span> not <span style=\"color:lightseagreen\">perish</span> from this <span style=\"color:chartreuse\">earth</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('Gettysburg.txt', 'r')\n",
    "Gettysburg = file.read()\n",
    "file2 = open(\"Dec_of_Independence.txt\", 'r')\n",
    "Declaration = file2.read()\n",
    "file3 = open(\"MLK_Dream.txt\", 'r')\n",
    "MLK = file3.read()\n",
    "file4 = open(\"JFK_Inauguration.txt\", 'r')\n",
    "JFK = file4.read()\n",
    "\n",
    "speechDocs = collections.OrderedDict()\n",
    "speechDocs[\"s1\"] = Gettysburg\n",
    "speechDocs[\"s2\"] = Declaration\n",
    "speechDocs[\"s3\"] = MLK\n",
    "speechDocs[\"s4\"] = JFK\n",
    "\n",
    "speechCorp = make_word_matrix(speechDocs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "ResultsSpeech = LDA(3, speechCorp, 0.1)\n",
    "ResultsSpeech2 = LDA(10, speechCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostCommon(ResultsSpeech2[0][3], speechCorp[1], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Assign Words to the Topic in which they Occur with the Highest Probabiliy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colorCode(beta, wordList, inputWords):\n",
    "    \n",
    "    #Split text, convert to lowercase and remove punctuation and stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "    text = inputWords.translate(table)\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(word) for word in words]\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    \n",
    "    \n",
    "    k = len(words)\n",
    "    topicWords = np.zeros((1,k))\n",
    "    topicWords = pd.DataFrame(topicWords,  columns=words)\n",
    "\n",
    "    for i in range(0, k):\n",
    "        document = np.array(betaDF.loc[:,words[i]])\n",
    "        topicWords.iloc[:,i] = np.argmax(document)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = colorCode(ResultsSpeech2[0][3], speechCorp[1], GSample).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ii. Movie Data*\n",
    "\n",
    "In addition to the topic creation for the historical documents, the LDA algorithm was used to cluster movies based on user ratings. This example used the MovieLens data set from the GroupLens website. (http://grouplens.org/datasets/movielens/)  This data contains user ratings for different movies, and we use the data to determine clusters of movies based on which movies the users prefer. A movie is preferred if a user rated it 4 or 5 (out of 5). The data was then reduced to only the information about users who had at least 50 preferred movies. In this example, the users can be thought of as the documents and their preferred movies are the words. The corpus is the collection of users. LDA will create topics for these movies that should cluster movies together based on which user preferred which movies. The LDA algorithm was used to create five clusters of movies. (Final only assigns a movie to one cluster?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "import pandas as pd\n",
    "Ratings = pd.read_csv(\"RatingsDataClean.csv\")\n",
    "movieData = pd.read_csv(\"MovieReferenceTable.csv\", encoding='cp1252')\n",
    "\n",
    "\n",
    "#making a dictionary for the training and testing data\n",
    "Users = list(set(Ratings['userID']))\n",
    "RatingsDict = {}\n",
    "for i in range(0, len(Users)):\n",
    "    movies = list(map(str, list(pd.DataFrame(Ratings[Ratings['userID'] == Users[i]])['movieID'])))\n",
    "    RatingsDict[i] = ' '.join(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#running LDA analysis to get parameters for movie training data\n",
    "import random\n",
    "\n",
    "random.seed(77)\n",
    "movieMatrix = make_word_matrix(RatingsDict, 1)\n",
    "movieOutput = LDA(5, movieMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieTopics = mostCommon(movieOutput[0][3], movieMatrix[1], 15)\n",
    "movieList = []\n",
    "for i in range(0,5):\n",
    "    movieList.append(list(np.array(movieTopics[i].index, dtype = int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betaDF = movieOutput[0][3]\n",
    "\n",
    "k = len(movieMatrix[1])\n",
    "topicWords = np.zeros((1,k))\n",
    "topicWords = pd.DataFrame(topicWords,  columns=movieMatrix[1])\n",
    "\n",
    "for i in range(0, k):\n",
    "    document = np.array(betaDF[:,i])\n",
    "    topicWords.iloc[:,i] = np.argmax(document)\n",
    "\n",
    "topicWords = topicWords.T\n",
    "\n",
    "topicWords['movieID'] = topicWords.index.astype(int)\n",
    "\n",
    "movieData = topicWords.merge(movieData, on = \"movieID\", how = \"left\")\n",
    "movieData.columns = ['cluster', 'movieID', 'UserID', \"Name\", \"Genre\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster 1\n",
    "movieData[movieData['cluster'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cluster appears to be mostly composed of children's and family movies. One exception is Freeway, a rated R comedy, crime and thriller movie. (http://www.imdb.com/title/tt0116361/?ref_=fn_al_tt_1). In addition, Golden Eye (http://www.imdb.com/title/tt0113189/?ref_=fn_al_tt_1) is a James Bond movie that does not seem to very similar to the other movies in this cluster. Freeway and Golden Eye are probably movies enjoyed by similar viewers. So it is possible that this cluster is the reuslt of parents who watch a lot of children's movies but also enjoys action movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster 2\n",
    "movieData[movieData['cluster'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster has fewer children's movies than Cluster 1. There are a lot of drama and thriller movies in this cluster. These could be rated by users who enjoy those types of movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster 3\n",
    "movieData[movieData['cluster'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster also consistents of many children's movies. The movies that are not children's movies are the comedies Ed's Next Move, Bottle Rocket and That Thing You Do! This could be another instance of parents rating movies,  but instead of action movies they enjoy watching comedies when their children are not around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster 4\n",
    "movieData[movieData['cluster'] == 3].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 3 only had one children's movie, but contains many comedies. These could be the movies enjoyed by users who enjoy watching humorous movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster 5\n",
    "movieData[movieData['cluster'] == 4].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 5 also has a lot of children's movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comment on why children's movies are everywhere? probably rated a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V.\tFurther Research**\n",
    "\n",
    "One area in which we would like to expand our research and improve our project is in the approximation of the posterior. In the implementation for this project, variational inference was utilized to estimate the intractable posterior distribution. However, there are other methods that can be used to estimate the posterior. These include Gibbs sampling and Metropolis-Hastings. It is possible that these approximations might increase the speed of the implementation. \n",
    "Another area to expand on is the determination of the number of topics. This is an ongoing area of debate and research at the moment. The R implementation of LDA provides different measures to determine the number of topics, and these are something that we can look into adding to our code.\n",
    "\n",
    "In order to improve the topic definitions, we want to investigate weighting terms by the number of times that they appear in the documents. Many of the results  in this report contain topics that have the same words in each of them. It is sensical that the same word may appear in documents on similar topics, such as American historical documents, but if a word appears many times in each document, that word will not be very helpful in defining topics. Another way to impropve topic definition would be to implement smoothing into the algorithm. If a word only occurs once in a corpus, it will have a very small probability. However, this word could be important in defining the topic for a particular document. Smoothing would ensure that every word would be assigned a positive probability. \n",
    "\n",
    "In addition, we want to continue to use the algorithms to situations beyond text corpora. The movie data example in this paper is one example of how LDA can be used for something beyond text data. The algorithm can be used for any situation that has the same structure as text corpora. The possibilities for LDA applications are endless, and we hope to explore more of these situations in the future. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI. References**\n",
    "\n",
    "1.\tColorado Reed, Latent Dirichlet Allocation: Towards a Deeper Understanding, January 2012, o\n",
    "bphio.us/pdfs/lda_tutorial.pdf. \n",
    "2.\tDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan, Latent Dirichlet Allocation, Journal of \n",
    "Machine Learning Research 3, 2003, pg. 993-1022.\n",
    "3. Internet Movie Database, www.imbd.com.\n",
    "4.\tMax Sklar, Fast MLE Computation for the Dirichlet Multinomial, May 2014.\n",
    "5.\tF. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "6.\tThomas P. Minka, Estimating a Dirichlet Distribution, www.msr-waypoint.com.\n",
    "7. Gettysburg Address: http://www.gutenberg.org/cache/epub/4/pg4.txt.\n",
    "8.  Declaration of Independence: http://www.gutenberg.org/files/16780/16780-h/16780-h.html\n",
    "9.  MLK Speech:  http://www.americanrhetoric.com/speeches/mlkihaveadream.htm\n",
    "10.  JFK Speech: http://www.americanrhetoric.com/speeches/jfkinaugural.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
