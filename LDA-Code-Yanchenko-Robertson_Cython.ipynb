{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 The quick brown fox\n",
      "s2 Brown fox jumps over the jumps jumps jumps\n",
      "s3 The the the lazy dog elephant.\n",
      "s4 The the the the the dog peacock lion tiger elephant\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "s1 = \"The quick brown fox\"\n",
    "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "s3 = \"The the the lazy dog elephant.\"\n",
    "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "docs = collections.OrderedDict()\n",
    "docs[\"s1\"] = s1\n",
    "docs[\"s2\"] = s2\n",
    "docs[\"s3\"] = s3\n",
    "docs[\"s4\"] = s4 \n",
    "for k, v in docs.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to make Corpus Matrix**\n",
    "\n",
    "Function returns a list, 0th element is a 3 dimensional array, each element corresponds to a document in the corpus where the columns are the unique words in that document and the rows are the words in that document (stop words removed).  Second element in list is the column names for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import stop_words\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "\n",
    "#Function to make document, word matricies for LDA#\n",
    "def make_word_matrix(corpus, needToSplit):\n",
    "    \n",
    "    \n",
    "    #Check to make sure NeedToSplit argument is 0 or 1\n",
    "    if needToSplit != 0 and needToSplit != 1:\n",
    "        print(\"NeedToSplit argument must be 0 or 1\")\n",
    "        return;\n",
    "    \n",
    "    \n",
    "    #Define stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    \n",
    "    \n",
    "    #Initialize stemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    #Define list to store corpus data#\n",
    "    c = []\n",
    "    #Define list to store order of words for each document#\n",
    "    wordOrder = []\n",
    "    #Define table to remove punctuation\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "    M = len(corpus)\n",
    "    \n",
    "    #Check to make sure that dictionary isn't empty#\n",
    "    if M ==0:\n",
    "        print(\"Input dictionary is empty\")\n",
    "        return;\n",
    "    \n",
    "    removePunc = string.punctuation\n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        if isinstance(corpus[i], str) != True:\n",
    "            print(\"Corpus input is not a string\")\n",
    "            return;\n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "        \n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "\n",
    "        #Remove stop words#\n",
    "        text = [word for word in words if word not in stopWords]\n",
    "        # stem tokens\n",
    "        text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Find number of unique words in each document#\n",
    "        Vwords = list(set(text))\n",
    "        wordOrder.append(Vwords)\n",
    "\n",
    "    #Find unique words in the corpus, this is the vocabulary#    \n",
    "    wordOrder = list(set(x for l in wordOrder for x in l))\n",
    "    wordOrder = sorted(wordOrder)\n",
    "\n",
    "    #Find the number of unique words in the corpus vocabulary#\n",
    "    V = len(wordOrder)\n",
    "    \n",
    "    #For each document in docs, caculate frequency of the words#\n",
    "    for i in corpus:\n",
    "        \n",
    "        #if the documents in the corpus are contained in a single string\n",
    "        if needToSplit == 1:\n",
    "            #Remove punctuation \n",
    "            text = corpus[i].translate(table)\n",
    "            #Splits string by blankspace and goes to lower case#\n",
    "            words = text.lower().split()\n",
    "            #Remove stop words#\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        else:\n",
    "            #Remove punctuation\n",
    "            for j in range(0, len(removePunc)):\n",
    "                while removePunc[j] in corpus[i]: \n",
    "                    corpus[i].remove(removePunc[j])    \n",
    "            \n",
    "            #convert everything to a lower case\n",
    "            corpus[i] = list(map(lambda x:x.lower(),corpus[i]))\n",
    "            words = corpus[i]\n",
    "            \n",
    "            #remove stop words\n",
    "            text = [word for word in words if word not in stopWords]\n",
    "            #Stemming\n",
    "            text = [p_stemmer.stem(i) for i in text]\n",
    "        #Find total number of words in each document#\n",
    "        N = len(text)\n",
    "\n",
    "        #Create matrix to store words for each document#\n",
    "        wordsMat = np.zeros((N, V))\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            v = wordOrder.index(word)\n",
    "            wordsMat[count, v] = 1\n",
    "            count = count + 1\n",
    "        c.append(wordsMat)\n",
    "\n",
    "    return [c, wordOrder, M] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])],\n",
       " ['brown',\n",
       "  'dog',\n",
       "  'eleph',\n",
       "  'fox',\n",
       "  'jump',\n",
       "  'lazi',\n",
       "  'lion',\n",
       "  'peacock',\n",
       "  'quick',\n",
       "  'tiger'],\n",
       " 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusMatrix = make_word_matrix(docs, 1)\n",
    "corpusMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E-Step:** This function uses variational inference to perform the E step in the EM algorithm to estimate the paramteters in the model.  The output of this function are the matricies gamma and phi, where gamma (k vector) is the Dirichlet paramteters and the matrix phi (N x k, where k is the number of topics) are the multinomial paramters.  See page 1004 of paper for derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Estep(k, d, alpha, beta, corpusMatrix, tol):    \n",
    "    \n",
    "    #storing the total number of words and the number of unique words\n",
    "    N = corpusMatrix[0][d].shape[0]\n",
    "    V = corpusMatrix[0][d].shape[1]\n",
    "    \n",
    "    #initialize phi and gamma\n",
    "    oldPhi  = np.full(shape = (N,k), fill_value = 1/k)\n",
    "    gamma = alpha + N/k\n",
    "    newPhi = oldPhi\n",
    "    converge = 0 \n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while converge == 0:\n",
    "        newPhi  = np.zeros(shape = (N,k))\n",
    "        for n in range(0, N):\n",
    "            for i in range(0,k):\n",
    "                newPhi[n,i] = (beta[i, list(corpusMatrix[0][d][n,:]).index(1)])*np.exp(scipy.special.psi(gamma[i]))\n",
    "        newPhi = newPhi/np.sum(newPhi, axis = 1)[:, np.newaxis] #normalizing the rows of new phi\n",
    "\n",
    "        for i in range(0,k):\n",
    "            gamma[i] = alpha[i] + np.sum(newPhi[:, i]) #updating gamma\n",
    "\n",
    "\n",
    "        criteria = (1/(N*k)*np.sum((newPhi - oldPhi)**2))**0.5\n",
    "        if criteria < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            oldPhi = newPhi\n",
    "            count = count +1\n",
    "            converge = 0\n",
    "    return (newPhi, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "np.random.seed(7)\n",
    "alphaOld = 10*np.random.rand(k)\n",
    "V = corpusMatrix[0][0].shape[1]\n",
    "betaOld = np.random.rand(k, V)\n",
    "betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for i in range(0,100):\n",
    "    phi, gamma = Estep(k, 0, alphaOld, betaOld, corpusMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M Step:** In the E step above, we maximized a lower bound with respect to gamma and phi, and in the M step, for fixed values of these variational parameters, we maximize the lower bound of the log likelihood with repsect to alpha and beta to update these values (combined, these two steps give approximate empirical Bayes estimates for the LDA model).  See pg. 1006 and appendix A.2 for derivation.  \n",
    "\n",
    "The alphaUpdate() function uses the linear Newton-Rhapson method to update the Dirichlet parameters, alpha, while the Mstep() function maximizes for alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "def alphaUpdate(k, M, alphaOld, gamma, tol):\n",
    "    h = np.zeros(k)\n",
    "    g = np.zeros(k)\n",
    "    alphaNew = np.zeros(k)\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(0, M):\n",
    "                docSum += scipy.special.psi(gamma[d][i]) - scipy.special.psi(np.sum(gamma[d]))\n",
    "            g[i] = M*(scipy.special.psi(sum(alphaOld)) - scipy.special.psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*scipy.special.polygamma(1, alphaOld[i])\n",
    "        z =  -scipy.special.polygamma(1, np.sum(alphaOld))\n",
    "        c = np.sum(g/h)/(1/z + np.sum(1/h))\n",
    "        step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            alphaNew = alphaOld + step\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol):\n",
    "    #Calculate beta#\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(0,M):\n",
    "                Nd = corpusMatrix[0][d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi[d][n,i]*corpusMatrix[0][d][n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    ##Update ALPHA##\n",
    "    alphaNew = alphaUpdate(k, M, alphaOld, gamma, tol)\n",
    "    return(alphaNew, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that gsl and scipy results are similar\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n",
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! python setup.py build\n",
    "! python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "\n",
      "#from cython_gsl cimport *\n",
      "import cython_gsl\n",
      "import scipy\n",
      "\n",
      "print(gsl_sf_psi_n(1, 10))\n",
      "                 ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Megan Robertson\\.ipython\\cython\\_cython_magic_17ef91aa9981bf023b6149da5ea5f586.pyx:6:18: undeclared name not builtin: gsl_sf_psi_n\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "import cython_gsl\n",
      "import scipy\n",
      "\n",
      "print(gsl_sf_psi_n(1, 10))\n",
      "print(scipy.special.polygamma(1, 10))\n",
      "print(gsl_sf_psi(10))\n",
      "               ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\Megan Robertson\\.ipython\\cython\\_cython_magic_17ef91aa9981bf023b6149da5ea5f586.pyx:8:16: undeclared name not builtin: gsl_sf_psi\n"
     ]
    }
   ],
   "source": [
    "%%cython -lgsl\n",
    "\n",
    "#from cython_gsl cimport *\n",
    "import cython_gsl\n",
    "import scipy\n",
    "\n",
    "print(gsl_sf_psi_n(1, 10))\n",
    "print(scipy.special.polygamma(1, 10))\n",
    "print(gsl_sf_psi(10))\n",
    "print(scipy.special.psi(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cython code for updateing $\\alpha$ and $\\beta$\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a -lgsl\n",
    "from cython_gsl cimport *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython -a -lgsl\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import scipy\n",
    "from cython_gsl cimport *\n",
    "\n",
    "#Update alpha using linear Newton-Rhapson Method#\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:] cy_alphaUpdate(int k, int M, double[:] alphaOld, double[:, :] gamma, double tol):\n",
    "    cdef double[:] h = np.zeros(k)\n",
    "    cdef double[:] g = np.zeros(k)\n",
    "    cdef double[:] alphaNew = np.zeros(k)\n",
    "\n",
    "    cdef int converge\n",
    "    cdef int i, d\n",
    "    cdef double docSum\n",
    "    cdef double[:] step = np.zeros(k)\n",
    "    cdef double c, s1, s2\n",
    "\n",
    "    cdef double alpha_sum = 0\n",
    "    for i in range(k):\n",
    "        alpha_sum += alphaOld[i]\n",
    "\n",
    "    converge = 0\n",
    "    while converge == 0:\n",
    "        z = -gsl_sf_psi_n(1, alpha_sum)\n",
    "\n",
    "        s1 = 0\n",
    "        s2 = 1.0/z\n",
    "        for i in range(0, k):\n",
    "            docSum = 0\n",
    "            for d in range(M):\n",
    "                docSum += gsl_sf_psi(gamma[d,i]) - gsl_sf_psi(sum(gamma[d]))\n",
    "            g[i] = M*(gsl_sf_psi(alpha_sum) - gsl_sf_psi(alphaOld[i])) + docSum\n",
    "            h[i] = M*gsl_sf_psi_n(1, alphaOld[i])\n",
    "            \n",
    "            s1 += g[i]/h[i]\n",
    "            s2 += 1.0/h[i]\n",
    "        c = s1/s2\n",
    "\n",
    "        for i in range(k):\n",
    "            step[i] = (g[i] - c)/h[i]\n",
    "        # step = (g - c)/h\n",
    "        if np.linalg.norm(step) < tol:\n",
    "            converge = 1\n",
    "        else:\n",
    "            converge = 0\n",
    "            for i in range(k):\n",
    "                alphaNew[i] = alphaOld[i] + step[i]\n",
    "            alphaOld = alphaNew\n",
    "\n",
    "    return alphaNew\n",
    "\n",
    "@cython.wraparound(False)\n",
    "@cython.boundscheck(False)\n",
    "cpdef double[:, :] cy_betaUpdate(int k, int M, long[:] phi_1, double[:, :, :] phi_2, double[:, :] gamma, \n",
    "                              long[:] c_1, double[:, :, :] c_2, double tol):\n",
    "\n",
    "    cdef int i, j, d, n\n",
    "    cdef int Nd\n",
    "    cdef double wordSum\n",
    "\n",
    "    #Calculate beta#\n",
    "    cdef int V = c_2[0].shape[1]\n",
    "    cdef double[:, :] beta = np.zeros(shape = (k,V))\n",
    "\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,V):\n",
    "            wordSum = 0\n",
    "            for d in range(M):\n",
    "                Nd = c_1[d] # c[d].shape[0]\n",
    "                for n in range(0, Nd):\n",
    "                    wordSum += phi_2[d, n,i]*c_2[d, n,j]\n",
    "            beta[i,j] = wordSum\n",
    "    #Normalize the rows of beta#\n",
    "    beta = beta/np.sum(beta, axis = 1)[:, np.newaxis]\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Cython functions against original functions\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare funciton arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "M = 4\n",
    "phi = [np.array([[ 0.38340797,  0.30327435,  0.31331768],\n",
    "       [ 0.5703127 ,  0.17742331,  0.25226399],\n",
    "       [ 0.3176045 ,  0.43558053,  0.24681497]]), np.array([[ 0.55072546,  0.18583053,  0.26344401],\n",
    "       [ 0.30048528,  0.44698122,  0.2525335 ],\n",
    "       [ 0.32099179,  0.28895213,  0.39005608],\n",
    "       [ 0.32099179,  0.28895213,  0.39005608],\n",
    "       [ 0.32099179,  0.28895213,  0.39005608],\n",
    "       [ 0.32099179,  0.28895213,  0.39005608]]), np.array([[ 0.0623253 ,  0.24856995,  0.68910475],\n",
    "       [ 0.63063084,  0.08148213,  0.28788703],\n",
    "       [ 0.12858014,  0.39114008,  0.48027977]]), np.array([[ 0.66366397,  0.08257639,  0.25375964],\n",
    "       [ 0.77627072,  0.10684157,  0.11688771],\n",
    "       [ 0.75940435,  0.23665379,  0.00394186],\n",
    "       [ 0.02553949,  0.62915851,  0.345302  ],\n",
    "       [ 0.14168349,  0.41504784,  0.44326867]])]\n",
    "gamma = np.array([np.array([ 9.71859516,  6.63758257,  7.30234283]), np.array([ 10.58244789,   7.50992466,   8.56614802]), np.array([ 9.26880627,  6.44249655,  7.94721775]), np.array([ 10.813832  ,   7.19158249,   7.65310607])])\n",
    "alphaOld = np.array([ 8.44726999 , 5.72130438,  6.48994619])\n",
    "c = np.array([np.array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]]), np.array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]]), np.array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]), np.array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
    "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])])\n",
    "tol = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = len(phi)\n",
    "phi_1 = np.array([p_.shape[0] for p_ in phi]).astype('int')\n",
    "n = max(phi_1)\n",
    "p = phi[0].shape[1]\n",
    "phi_2 = np.zeros((r, n, p))\n",
    "for i, j in enumerate(phi_1):\n",
    "    phi_2[i, :j, :] = phi[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = len(c)\n",
    "c_1 = np.array([c_.shape[0] for c_ in c]).astype('int')\n",
    "n = max(c_1)\n",
    "p = c[0].shape[1]\n",
    "c_2 = np.zeros((r, n, p))\n",
    "for i, j in enumerate(c_1):\n",
    "    c_2[i, :j, :] = c[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "alphaNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betaNew = np.array(cy_betaUpdate(k, M, phi_1, phi_2, gamma, c_1, c_2, tol))\n",
    "betaNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any speed-up?\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit a, b = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "betaNew = np.array(cy_betaUpdate(k, M, phi_1, phi_2, gamma, c_1, c_2, tol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Function:**\n",
    "Finally, we implement the entire Latent Dirichlet Allocation method in the LDA function, which takes as its arguments k (the number of topics), D (the number of documents in the corpus), a corpus matrix (the output from make_word_matrix above) and a tolerance (which sets the convergence criteria for the while loops).  For each document d, the function runs until the alpha or beta parameters converge, by first running the E step and then the M step for each document separately.  The final values of phi, gamma, alpha and beta are returned for all D documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDA(k, corpusMatrix, tol):\n",
    "\n",
    "    # create rectangular matrices for c\n",
    "    c = corpusMatrix[0]\n",
    "    r_c = len(c)\n",
    "    c_1 = np.array([c_.shape[0] for c_ in c]).astype('int')\n",
    "    n_c = max(c_1)\n",
    "    p_c = c[0].shape[1]\n",
    "    c_2 = np.zeros((r_c, n_c, p_c))\n",
    "    for i, j in enumerate(c_1):\n",
    "        c_2[i, :j, :] = c[i]\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = 10*np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    while converge == 0:\n",
    "        phi = []\n",
    "        gamma = []\n",
    "        #looping through the number of documents\n",
    "        for d in range(0,M): #M is the number of documents\n",
    "            phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "            phi.append(phiT)\n",
    "            gamma.append(gammaT)\n",
    "            \n",
    "        # create rectangular matrices for phi\n",
    "        r = len(phi)\n",
    "        phi_1 = np.array([p_.shape[0] for p_ in phi]).astype('int')\n",
    "        n = max(phi_1)\n",
    "        p = phi[0].shape[1]\n",
    "        phi_2 = np.zeros((r, n, p))\n",
    "        for i, j in enumerate(phi_1):\n",
    "            phi_2[i, :j, :] = phi[i]\n",
    "\n",
    "                \n",
    "        gamma = np.array(gamma)\n",
    "\n",
    "        alphaNew = np.array(cy_alphaUpdate(k, M, alphaOld, gamma, tol))\n",
    "        betaNew = np.array(cy_betaUpdate(k, M, phi_1, phi_2, gamma, c_1, c_2, tol))\n",
    "    \n",
    "        if np.linalg.norm(alphaOld - alphaNew) < tol or np.linalg.norm(betaOld - betaNew) < tol:\n",
    "            converge =1\n",
    "        else: \n",
    "            converge =0\n",
    "            alphaOld = alphaNew\n",
    "            betaOld = betaNew\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LDA() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-29d3871aae05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\nLDA(3, corpusMatrix, 0.1, 1)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Megan Robertson\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Megan Robertson\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Megan Robertson\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Megan Robertson\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: LDA() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "LDA(3, corpusMatrix, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to return the most probable words\n",
    "#p is the number of words you want returned for each topic\n",
    "def mostCommon(beta, wordList, p):\n",
    "    k = beta.shape[0]\n",
    "    topicWords = []\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    for i in range(0, k):\n",
    "        document = betaDF.loc[i,:]\n",
    "        document.sort(1, ascending = 0)\n",
    "        mostCommon = pd.DataFrame(document[0:p])\n",
    "        topicWords.append(mostCommon)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "docsTest = collections.OrderedDict()\n",
    "docsTest[\"s1\"] = doc_a\n",
    "docsTest[\"s2\"] = doc_b\n",
    "docsTest[\"s3\"] = doc_c\n",
    "docsTest[\"s4\"] = doc_d \n",
    "docsTest[\"s5\"] = doc_e \n",
    "for k, v in docsTest.items():\n",
    "    print(k,v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cTest = make_word_matrix(docsTest, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State of the Union Example**\n",
    "\n",
    "In order to determine the initial performance time of our algorithm, we implemented LDA on a small corpus of eight documents. These documents are each of the State of the Union Adresses delivered by President Clinton during his time in office. These were found in the nltk Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"state_union\")\n",
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a diciontary of the state of the Union Addresses for Clinton\n",
    "fileNames = state_union.fileids()\n",
    "Clinton = fileNames[50:58]\n",
    "\n",
    "ClintonSOTU = {}\n",
    "for i in range(0, len(Clinton)):\n",
    "    ClintonSOTU[Clinton[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(Clinton[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileNames = state_union.fileids()\n",
    "SOTU = fileNames\n",
    "\n",
    "PresSOTU = {}\n",
    "for i in range(0, len(SOTU)):\n",
    "    PresSOTU[SOTU[i].rsplit('.', 1)[0]] = list(nltk.corpus.state_union.words(SOTU[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "PresCorp = make_word_matrix(PresSOTU, 0)\n",
    "ResultsAll = LDA(5, PresCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(7)\n",
    "ClintCorp = make_word_matrix(ClintonSOTU, 0)\n",
    "Results = LDA(3, ClintCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mostCommon(ResultsAll[0][3], PresCorp[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bookNames = gutenberg.fileids()\n",
    "indicies = [0,7,15]\n",
    "books = [bookNames[i] for i in indicies]\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "booksInput = {}\n",
    "for i in range(0, len(books)):\n",
    "    booksInput[books[i].rsplit('.', 1)[0]] = list(nltk.corpus.gutenberg.words(books[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(77)\n",
    "bookCorp = make_word_matrix(booksInput, 0)\n",
    "print(\"LDA\")\n",
    "ResultsBooks = LDA(5, bookCorp, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "http://www.gutenberg.org/cache/epub/4/pg4.txt\n",
    "http://www.gutenberg.org/files/16780/16780-h/16780-h.html\n",
    "http://www.americanrhetoric.com/speeches/mlkihaveadream.htm\n",
    "http://www.americanrhetoric.com/speeches/jfkinaugural.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gettysburg = \"\"\"Four score and seven years ago, our fathers brought forth\n",
    "upon this continent a new nation:  conceived in liberty, and\n",
    "dedicated to the proposition that all men are created equal.\n",
    "\n",
    "Now we are engaged in a great civil war. . .testing whether\n",
    "that nation, or any nation so conceived and so dedicated. . .\n",
    "can long endure.  We are met on a great battlefield of that war.\n",
    "\n",
    "We have come to dedicate a portion of that field as a final resting place\n",
    "for those who here gave their lives that this nation might live.\n",
    "It is altogether fitting and proper that we should do this.\n",
    "\n",
    "But, in a larger sense, we cannot dedicate. . .we cannot consecrate. . .\n",
    "we cannot hallow this ground.  The brave men, living and dead,\n",
    "who struggled here have consecrated it, far above our poor power\n",
    "to add or detract.  The world will little note, nor long remember,\n",
    "what we say here, but it can never forget what they did here.\n",
    "\n",
    "It is for us the living, rather, to be dedicated here to the unfinished\n",
    "work which they who fought here have thus far so nobly advanced.\n",
    "It is rather for us to be here dedicated to the great task remaining\n",
    "before us. . .that from these honored dead we take increased devotion\n",
    "to that cause for which they gave the last full measure of devotion. . .\n",
    "that we here highly resolve that these dead shall not have died in vain. . .\n",
    "that this nation, under God, shall have a new birth of freedom. . .\n",
    "and that government of the people. . .by the people. . .for the people. . .\n",
    "shall not perish from this earth.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Declaration = \"\"\"The unanimous Declaration of the thirteen united States of America\n",
    "\n",
    "When in the Course of human events, it becomes necessary for one people to dissolve the political bands \n",
    "which have connected them with another, and to assume, among the Powers of the earth, the separate and equal \n",
    "station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of \n",
    "mankind requires that they should \n",
    "declare the causes which impel them to the separation.\n",
    "\n",
    "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their \n",
    "Creator with certain unalienable Rights, that among these are Life, Liberty, and the pursuit of Happiness.\n",
    "—That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent \n",
    "of the governed,—That whenever any Form of Government becomes destructive of these ends, it is the Right \n",
    "of the People to alter or to abolish it, and to institute new Government, laying its foundation on such \n",
    "principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety \n",
    "and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for \n",
    "light and transient causes; and accordingly all experience hath shown, \n",
    "that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the \n",
    "forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same \n",
    "Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off \n",
    "such Government, and to provide new Guards for their future security.—Such has been the patient sufferance of these \n",
    "Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. \n",
    "The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in \n",
    "direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to \n",
    "a candid world.\n",
    "\n",
    "He has refused his Assent to Laws, the most wholesome and necessary for the public good.\n",
    "He has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their \n",
    "operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.\n",
    "He has refused to pass other Laws for the accommodation of large districts of people, unless those people would \n",
    "relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.\n",
    "He has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of \n",
    "their Public Records, for the sole purpose of fatiguing them into compliance with his measures.\n",
    "He has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights \n",
    "of the people.\n",
    "He has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative \n",
    "Powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining \n",
    "in the mean time exposed to all the dangers of invasion from without, and convulsions within.\n",
    "He has endeavoured to prevent the population of these States; for that purpose obstructing the Laws of \n",
    "Naturalization of Foreigners; refusing to pass others to encourage their migration hither, and raising the \n",
    "conditions of new Appropriations of Lands.\n",
    "He has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary Powers.\n",
    "He has made judges dependent on his Will alone, for the tenure of their offices, and the amount and payment \n",
    "of their salaries.\n",
    "He has erected a multitude of New Offices, and sent hither swarms of Officers to harass our People, and eat \n",
    "out their substance.\n",
    "He has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.\n",
    "He has affected to render the Military independent of and superior to the Civil Power.\n",
    "He has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged \n",
    "by our laws; giving his Assent to their Acts of pretended legislation:\n",
    "For quartering large bodies of armed troops among us:\n",
    "For protecting them, by a mock Trial, from Punishment for any Murders which they should commit on the \n",
    "Inhabitants of these States:\n",
    "For cutting off our Trade with all parts of the world:\n",
    "For imposing taxes on us without our Consent:\n",
    "For depriving us, in many cases, of the benefits of Trial by Jury:\n",
    "For transporting us beyond Seas to be tried for pretended offences:\n",
    "For abolishing the free System of English Laws in a neighbouring Province, establishing therein an \n",
    "Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit \n",
    "instrument for introducing the same absolute rule into these Colonies:\n",
    "For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms \n",
    "of our Governments:\n",
    "For suspending our own Legislatures, and declaring themselves invested with Power to legislate for us \n",
    "in all cases whatsoever.\n",
    "He has abdicated Government here, by declaring us out of his Protection and waging War against us.\n",
    "He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.\n",
    "He is at this time transporting large armies of foreign mercenaries to compleat the works of death, \n",
    "desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled \n",
    "in the most barbarous ages, and totally unworthy of the Head of a civilized nation.\n",
    "He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their \n",
    "Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.\n",
    "He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of \n",
    "our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction \n",
    "of all ages, sexes and conditions.\n",
    "In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated \n",
    "Petitions have been answered only by repeated injury. A Prince, whose character is thus marked by every act \n",
    "which may define a Tyrant, is unfit to be the ruler of a free People.\n",
    "\n",
    "Nor have We been wanting in attention to our Brittish brethren. We have warned them from time to time of \n",
    "attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the \n",
    "circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, \n",
    "and we have conjured them by the ties of our common kindred to disavow these usurpations, which would \n",
    "inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice \n",
    "and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, \n",
    "and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.\n",
    "\n",
    "We, therefore, the Representatives of the United States of America, in General Congress, Assembled, \n",
    "appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and \n",
    "by the Authority of the good People of these Colonies, solemnly publish and declare, That these United \n",
    "Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all \n",
    "Allegiance to the British Crown, and that all political connection between them and the State of Great \n",
    "Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full \n",
    "Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and \n",
    "Things which Independent States may of right do. And for the support of this Declaration, with a firm \n",
    "reliance on the Protection of Divine Providence, we mutually pledge to each other our Lives, our Fortunes \n",
    "and our sacred Honor.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLK = \"\"\"I am happy to join with you today in what will go down in history as the greatest \n",
    "demonstration for freedom in the history of our nation.\n",
    "\n",
    "Five score years ago, a great American, in whose symbolic shadow we stand today, signed the \n",
    "Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions \n",
    "of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous \n",
    "daybreak to end the long night of their captivity.\n",
    "\n",
    "But one hundred years later, the Negro still is not free. One hundred years later, the life of the \n",
    "Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred \n",
    "years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. \n",
    "One hundred years later, the Negro is still languished in the corners of American society and finds himself \n",
    "an exile in his own land. And so we've come here today to dramatize a shameful condition.\n",
    "\n",
    "In a sense we've come to our nation's capital to cash a check. When the architects of our republic wrote \n",
    "the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory \n",
    "note to which every American was to fall heir. This note was a promise that all men, yes, black men as well \n",
    "as white men, would be guaranteed the \"unalienable Rights\" of \"Life, Liberty and the pursuit of Happiness.\" \n",
    "It is obvious today that America has defaulted on this promissory note, insofar as her citizens of color are \n",
    "concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check \n",
    "which has come back marked \"insufficient funds.\"\n",
    "\n",
    "But we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient \n",
    "funds in the great vaults of opportunity of this nation. And so, we've come to cash this check, a check that will \n",
    "give us upon demand the riches of freedom and the security of justice.\n",
    "\n",
    "We have also come to this hallowed spot to remind America of the fierce urgency of Now. This is no time to\n",
    "engage in the luxury of cooling off or to take the tranquilizing drug of gradualism. Now is the time to make \n",
    "real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to \n",
    "the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice \n",
    "to the solid rock of brotherhood. Now is the time to make justice a reality for all of God's children.\n",
    "\n",
    "It would be fatal for the nation to overlook the urgency of the moment. This sweltering summer of the Negro's \n",
    "legitimate discontent will not pass until there is an invigorating autumn of freedom and equality. Nineteen \n",
    "sixty-three is not an end, but a beginning. And those who hope that the Negro needed to blow off steam and \n",
    "will now be content will have a rude awakening if the nation returns to business as usual. And there will be \n",
    "neither rest nor tranquility in America until the Negro is granted his citizenship rights. The whirlwinds of \n",
    "revolt will continue to shake the foundations of our nation until the bright day of justice emerges.\n",
    "\n",
    "But there is something that I must say to my people, who stand on the warm threshold which leads into the \n",
    "palace of justice: In the process of gaining our rightful place, we must not be guilty of wrongful deeds. \n",
    "Let us not seek to satisfy our thirst for freedom by drinking from the cup of bitterness and hatred. We must \n",
    "forever conduct our struggle on the high plane of dignity and discipline. We must not allow our creative \n",
    "protest to degenerate into physical violence. Again and again, we must rise to the majestic heights of \n",
    "meeting physical force with soul force.\n",
    "\n",
    "The marvelous new militancy which has engulfed the Negro community must not lead us to a distrust of \n",
    "all white people, for many of our white brothers, as evidenced by their presence here today, have come to \n",
    "realize that their destiny is tied up with our destiny. And they have come to realize that their freedom \n",
    "is inextricably bound to our freedom.\n",
    "\n",
    "We cannot walk alone.\n",
    "\n",
    "And as we walk, we must make the pledge that we shall always march ahead.\n",
    "\n",
    "We cannot turn back.\n",
    "\n",
    "There are those who are asking the devotees of civil rights, \"When will you be satisfied?\" \n",
    "We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of police brutality.\n",
    "We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging \n",
    "in the motels of the highways and the hotels of the cities. *We cannot be satisfied as long as the \n",
    "negro's basic mobility is from a smaller ghetto to a larger one. We can never be satisfied as long as \n",
    "our children are stripped of their self-hood and robbed of their dignity by signs stating: \"For Whites Only.\" \n",
    "We cannot be satisfied as long as a Negro in Mississippi cannot vote and a Negro in New York believes he has \n",
    "nothing for which to vote. No, no, we are not satisfied, and we will not be satisfied until \"justice rolls \n",
    "down like waters, and righteousness like a mighty stream.\"¹\n",
    "\n",
    "I am not unmindful that some of you have come here out of great trials and tribulations. \n",
    "Some of you have come fresh from narrow jail cells. And some of you have come from areas where your \n",
    "quest -- quest for freedom left you battered by the storms of persecution and staggered by the winds of \n",
    "police brutality. You have been the veterans of creative suffering. Continue to work with the faith that \n",
    "unearned suffering is redemptive. Go back to Mississippi, go back to Alabama, go back to South Carolina, \n",
    "go back to Georgia, go back to Louisiana, go back to the slums and ghettos of our northern cities, knowing \n",
    "that somehow this situation can and will be changed.\n",
    "\n",
    "Let us not wallow in the valley of despair, I say to you today, my friends.\n",
    "\n",
    "And so even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream \n",
    "deeply rooted in the American dream.\n",
    "\n",
    "I have a dream that one day this nation will rise up and live out the true meaning of its creed: \"We hold \n",
    "these truths to be self-evident, that all men are created equal.\"\n",
    "\n",
    "I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former \n",
    "slave owners will be able to sit down together at the table of brotherhood.\n",
    "\n",
    "I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, \n",
    "sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.\n",
    "\n",
    "I have a dream that my four little children will one day live in a nation where they will not be judged by \n",
    "the color of their skin but by the content of their character.\n",
    "\n",
    "I have a dream today!\n",
    "\n",
    "I have a dream that one day, down in Alabama, with its vicious racists, with its governor having his lips \n",
    "dripping with the words of \"interposition\" and \"nullification\" -- one day right there in Alabama little \n",
    "black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.\n",
    "\n",
    "I have a dream today!\n",
    "\n",
    "I have a dream that one day every valley shall be exalted, and every hill and mountain shall be made \n",
    "low, the rough places will be made plain, and the crooked places will be made straight; \"and the glory \n",
    "of the Lord shall be revealed and all flesh shall see it together.\"\n",
    "\n",
    "This is our hope, and this is the faith that I go back to the South with.\n",
    "\n",
    "With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith, \n",
    "we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. \n",
    "With this faith, we will be able to work together, to pray together, to struggle together, to go to jail \n",
    "together, to stand up for freedom together, knowing that we will be free one day.\n",
    "\n",
    "And this will be the day -- this will be the day when all of God's children will be able to sing \n",
    "with new meaning:\n",
    "\n",
    "My country 'tis of thee, sweet land of liberty, of thee I sing.\n",
    "\n",
    "Land where my fathers died, land of the Pilgrim's pride,\n",
    "\n",
    "From every mountainside, let freedom ring!\n",
    "\n",
    " \n",
    "\n",
    "And if America is to be a great nation, this must become true.\n",
    "\n",
    "And so let freedom ring from the prodigious hilltops of New Hampshire.\n",
    "\n",
    "Let freedom ring from the mighty mountains of New York.\n",
    "\n",
    "Let freedom ring from the heightening Alleghenies of Pennsylvania.\n",
    "\n",
    "Let freedom ring from the snow-capped Rockies of Colorado.\n",
    "\n",
    "Let freedom ring from the curvaceous slopes of California.\n",
    "\n",
    " \n",
    "\n",
    "But not only that:\n",
    "\n",
    "Let freedom ring from Stone Mountain of Georgia.\n",
    "\n",
    "Let freedom ring from Lookout Mountain of Tennessee.\n",
    "\n",
    "Let freedom ring from every hill and molehill of Mississippi.\n",
    "\n",
    "From every mountainside, let freedom ring.\n",
    "\n",
    " \n",
    "\n",
    "And when this happens, and when we allow freedom ring, when we let it ring from every \n",
    "village and every hamlet, from every state and every city, we will be able to speed up that \n",
    "day when all of God's children, black men and white men, Jews and Gentiles, Protestants and \n",
    "Catholics, will be able to join hands and sing in the words of the old Negro spiritual:\n",
    "\n",
    "                Free at last! Free at last!\n",
    "\n",
    "                Thank God Almighty, we are free at last!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JFK = \"\"\"Vice President Johnson, Mr. Speaker, Mr. Chief Justice, President Eisenhower, Vice President Nixon, \n",
    "President Truman, reverend clergy, fellow citizens:\n",
    "\n",
    "We observe today not a victory of party, but a celebration of freedom -- symbolizing an end, as well as a \n",
    "beginning -- signifying renewal, as well as change. For I have sworn before you and Almighty God the same \n",
    "solemn oath our forebears prescribed nearly a century and three-quarters ago.\n",
    "\n",
    "The world is very different now. For man holds in his mortal hands the power to abolish all forms of human \n",
    "poverty and all forms of human life. And yet the same revolutionary beliefs for which our forebears fought \n",
    "are still at issue around the globe -- the belief that the rights of man come not from the generosity of \n",
    "the state, but from the hand of God.\n",
    "\n",
    "We dare not forget today that we are the heirs of that first revolution. Let the word go forth from this \n",
    "time and place, to friend and foe alike, that the torch has been passed to a new generation of Americans -- \n",
    "born in this century, tempered by war, disciplined by a hard and bitter peace, proud of our ancient heritage, \n",
    "and unwilling to witness or permit the slow undoing of those human rights to which this nation has always \n",
    "been committed, and to which we are committed today at home and around the world.\n",
    "\n",
    "Let every nation know, whether it wishes us well or ill, that we shall pay any price, bear any burden, \n",
    "meet any hardship, support any friend, oppose any foe, to assure the survival and the success of liberty.\n",
    "\n",
    "This much we pledge -- and more.\n",
    "\n",
    "To those old allies whose cultural and spiritual origins we share, we pledge the loyalty of faithful \n",
    "friends. United there is little we cannot do in a host of cooperative ventures. Divided there is little \n",
    "we can do -- for we dare not meet a powerful challenge at odds and split asunder.\n",
    "\n",
    "To those new states whom we welcome to the ranks of the free, we pledge our word that one form of colonial \n",
    "control shall not have passed away merely to be replaced by a far more iron tyranny. We shall not always \n",
    "expect to find them supporting our view. But we shall always hope to find them strongly supporting their \n",
    "own freedom -- and to remember that, in the past, those who foolishly sought power by riding the back of \n",
    "the tiger ended up inside.\n",
    "\n",
    "To those people in the huts and villages of half the globe struggling to break the bonds of mass misery, \n",
    "we pledge our best efforts to help them help themselves, for whatever period is required -- not because \n",
    "the Communists may be doing it, not because we seek their votes, but because it is right. If a free \n",
    "society cannot help the many who are poor, it cannot save the few who are rich.\n",
    "\n",
    "To our sister republics south of our border, we offer a special pledge: to convert our good words into \n",
    "good deeds, in a new alliance for progress, to assist free men and free governments in casting off the \n",
    "chains of poverty. But this peaceful revolution of hope cannot become the prey of hostile powers. Let \n",
    "all our neighbors know that we shall join with them to oppose aggression or subversion anywhere in the \n",
    "Americas. And let every other power know that this hemisphere intends to remain the master of its own house.\n",
    "\n",
    "To that world assembly of sovereign states, the United Nations, our last best hope in an age where the \n",
    "instruments of war have far outpaced the instruments of peace, we renew our pledge of support -- to \n",
    "prevent it from becoming merely a forum for invective, to strengthen its shield of the new and the \n",
    "weak, and to enlarge the area in which its writ may run.\n",
    "\n",
    "Finally, to those nations who would make themselves our adversary, we offer not a pledge but a request: \n",
    "that both sides begin anew the quest for peace, before the dark powers of destruction unleashed by \n",
    "science engulf all humanity in planned or accidental self-destruction.\n",
    "\n",
    "We dare not tempt them with weakness. For only when our arms are sufficient beyond doubt can we be \n",
    "certain beyond doubt that they will never be employed.\n",
    "\n",
    "But neither can two great and powerful groups of nations take comfort from our present course -- \n",
    "both sides overburdened by the cost of modern weapons, both rightly alarmed by the steady spread of \n",
    "the deadly atom, yet both racing to alter that uncertain balance of terror that stays the hand of mankind's final war.\n",
    "So let us begin anew -- remembering on both sides that civility is not a sign of weakness, and sincerity is \n",
    "always subject to proof. Let us never negotiate out of fear, but let us never fear to negotiate.\n",
    "\n",
    "Let both sides explore what problems unite us instead of belaboring those problems which divide us.\n",
    "\n",
    "Let both sides, for the first time, formulate serious and precise proposals for the inspection and control of \n",
    "arms, and bring the absolute power to destroy other nations under the absolute control of all nations.\n",
    "\n",
    "Let both sides seek to invoke the wonders of science instead of its terrors. Together let us explore the stars, \n",
    "conquer the deserts, eradicate disease, tap the ocean depths, and encourage the arts and commerce.\n",
    "\n",
    "Let both sides unite to heed, in all corners of the earth, the command of Isaiah -- to \"undo the heavy burdens, \n",
    "and [to] let the oppressed go free.\"\n",
    "\n",
    "And, if a beachhead of cooperation may push back the jungle of suspicion, let both sides join in \n",
    "creating a new endeavor -- not a new balance of power, but a new world of law -- where the strong are \n",
    "just, and the weak secure, and the peace preserved.\n",
    "\n",
    "All this will not be finished in the first one hundred days. Nor will it be finished in the first one \n",
    "thousand days; nor in the life of this Administration; nor even perhaps in our lifetime on this planet. \n",
    "But let us begin.\n",
    "\n",
    "In your hands, my fellow citizens, more than mine, will rest the final success or failure of our course. \n",
    "Since this country was founded, each generation of Americans has been summoned to give testimony to its \n",
    "national loyalty. The graves of young Americans who answered the call to service surround the globe.\n",
    "\n",
    "Now the trumpet summons us again -- not as a call to bear arms, though arms we need -- not as a call to \n",
    "battle, though embattled we are -- but a call to bear the burden of a long twilight struggle, year in \n",
    "and year out, \"rejoicing in hope; patient in tribulation,\"² a struggle against the common enemies of man: \n",
    "tyranny, poverty, disease, and war itself.\n",
    "\n",
    "Can we forge against these enemies a grand and global alliance, North and South, East and West, that \n",
    "can assure a more fruitful life for all mankind? Will you join in that historic effort?\n",
    "In the long history of the world, only a few generations have been granted the role of defending freedom \n",
    "in its hour of maximum danger. I do not shrink from this responsibility -- I welcome it. I do not believe \n",
    "that any of us would exchange places with any other people or any other generation. The energy, \n",
    "the faith, the devotion which we bring to this endeavor will light our country and all who serve it. \n",
    "And the glow from that fire can truly light the world.\n",
    "\n",
    "And so, my fellow Americans, ask not what your country can do for you; ask what you can do for \n",
    "your country.\n",
    "\n",
    "My fellow citizens of the world, ask not what America will do for you, but what together we \n",
    "can do for the freedom of man.\n",
    "\n",
    "Finally, whether you are citizens of America or citizens of the world, ask of us here the \n",
    "same high standards of strength and sacrifice which we ask of you. With a good conscience our \n",
    "only sure reward, with history the final judge of our deeds, let us go forth to lead the land we \n",
    "love, asking His blessing and His help, but knowing that here on earth God's work must truly be our own.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechDocs = collections.OrderedDict()\n",
    "speechDocs[\"s1\"] = Gettysburg\n",
    "speechDocs[\"s2\"] = Declaration\n",
    "speechDocs[\"s3\"] = MLK\n",
    "speechDocs[\"s4\"] = JFK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.random.seed(77)\n",
    "speechCorp = make_word_matrix(speechDocs, 1)\n",
    "#print(\"LDA\")\n",
    "#ResultsBooks = LDA(5, bookCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speechCorp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "\n",
    "ResultsSpeech = LDA(3, speechCorp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostCommon(ResultsSpeech[0][3], speechCorp[1], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Code Text by Topic\n",
    "<span style=\"color:red\">Topic 1</span> \n",
    "\n",
    "<span style=\"color:blue\">Topic 2</span> \n",
    "\n",
    "<span style=\"color:green\">Topic 3</span> \n",
    "\n",
    "If a specific word is in more than one topic, I assigned it to that topic that gave that word the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When in the <span style=\"color:blue\">Course</span>  of <span style=\"color:green\">human</span> <span style=\"color:red\">events</span> , it <span style=\"color:red\">becomes</span> <span style=\"color:green\">necessary</span> for <span style=\"color:green\">one</span> <span style=\"color:green\">people</span> to <span style=\"color:green\">dissolve</span> the <span style=\"color:green\">political</span> <span style=\"color:red\">bands</span> \n",
    "which have <span style=\"color:blue\">connected</span> them with <span style=\"color:green\">another</span>, and to <span style=\"color:blue\">assume</span>, <span style=\"color:blue\">among</span> the <span style=\"color:green\">Powers</span> of the <span style=\"color:green\">earth</span> , the <span style=\"color:green\">separate</span> and <span style=\"color:green\">equal</span> \n",
    "<span style=\"color:blue\">station</span> to which the <span style=\"color:blue\">Laws</span> of <span style=\"color:blue\">Nature</span> and of <span style=\"color:blue\">Nature's</span> <span style=\"color:green\">God</span> <span style=\"color:green\">entitle</span> them, a <span style=\"color:red\">decent</span> <span style=\"color:red\">respect</span> to the <span style=\"color:red\">opinions</span> of \n",
    "<span style=\"color:red\">mankind</span> <span style=\"color:green\">requires</span> that they should \n",
    "<span style=\"color:green\">declare</span> the <span style=\"color:red\">causes</span> which <span style=\"color:red\">impel</span> them to the <span style=\"color:green\">separation</span>.\n",
    "\n",
    "We <span style=\"color:blue\">hold</span> these <span style=\"color:green\">truths</span> to be <span style=\"color:blue\">self-evident</span>, that all <span style=\"color:blue\">men</span> are <span style=\"color:red\">created</span> <span style=\"color:green\">equal</span>, that they are <span style=\"color:blue\">endowed</span> by their \n",
    "<span style=\"color:green\">Creator</span> with <span style=\"color:blue\">certain unalienable Rights</span>, that <span style=\"color:blue\">among</span> these are <span style=\"color:blue\">Life</span>, <span style=\"color:green\">Liberty</span>, and the <span style=\"color:red\">pursuit</span> of <span style=\"color:blue\">Happiness</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decIntro = \"\"\"When in the Course of human events, it becomes necessary for one people to dissolve the political bands \n",
    "which have connected them with another, and to assume, among the Powers of the earth, the separate and equal \n",
    "station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of \n",
    "mankind requires that they should \n",
    "declare the causes which impel them to the separation.\n",
    "\n",
    "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their \n",
    "Creator with certain unalienable Rights, that among these are Life, Liberty, and the pursuit of Happiness.\"\"\"\n",
    "colorCode(ResultsSpeech[0][3], speechCorp[1], decIntro).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colorCode(beta, wordList, inputWords):\n",
    "    \n",
    "    #Split text, convert to lowercase and remove punctuation and stop words\n",
    "    stopWords = get_stop_words('english')\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "    text = inputWords.translate(table)\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(word) for word in words]\n",
    "    betaDF = pd.DataFrame(beta)\n",
    "    betaDF.columns = wordList\n",
    "    \n",
    "    \n",
    "    k = len(words)\n",
    "    topicWords = np.zeros((1,k))\n",
    "    topicWords = pd.DataFrame(topicWords,  columns=words)\n",
    "\n",
    "    for i in range(0, k):\n",
    "        document = np.array(betaDF.loc[:,words[i]])\n",
    "        topicWords.iloc[:,i] = np.argmax(document)\n",
    "    return(topicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostCommon(ResultsSpeech[0][3], speechCorp[1], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pip install --pre line-profiler\n",
    "#pip install psutil\n",
    "# pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating profile files for the construction of the word matrix and LDA for SOTU example\n",
    "%prun -q -D ClintonWordMatrix.prof make_word_matrix(ClintonSOTU,0)\n",
    "%prun -q -D ClintonLDA.prof LDA(3, ClintCorp, 0.1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#printing profile results\n",
    "import pstats\n",
    "wordMatrixProfile = pstats.Stats('ClintonWordMatrix.prof')\n",
    "LDAProfile = pstats.Stats('ClintonLDA.prof')\n",
    "wordMatrixProfile.print_stats()\n",
    "LDAProfile.print_stats()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movie Clustering** \n",
    "\n",
    "In addition to text analysis, LDA can be implemented for other problems that have the same structure as a text corpora. In this section, we use LDA in order to predict movies preferred by users of the site MovieLens.com. The goal is to predict another movie that the user likes based on the movies that they have already said that they prefer. The definition of preferred here is that the user rated the movie four or five, where five is the maxiumum possible rating. Users who gave a preferred rating to at least 50 movies were used. A training set of 2,535 users was used to fit the model. The model is then shown all but one of the preferred movies for the users in the test data, and this is used to determine the last movie that the user preferred. Thus, it is possible to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "import pandas as pd\n",
    "Ratings = pd.read_csv(\"RatingsDataClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#making a dictionary for the training and testing data\n",
    "Users = list(set(Ratings['userID']))\n",
    "RatingsDict = {}\n",
    "for i in range(0, len(Users)):\n",
    "    movies = list(map(str, list(pd.DataFrame(Ratings[Ratings['userID'] == Users[0]])['movieID'])))\n",
    "    RatingsDict[i] = ' '.join(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cy_alphaUpdate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-891b6a420411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmovieMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_word_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRatingsDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmovieOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovieMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-1f31541db9ed>\u001b[0m in \u001b[0;36mLDA\u001b[1;34m(k, corpusMatrix, tol)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0malphaNew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcy_alphaUpdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphaOld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mbetaNew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcy_betaUpdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cy_alphaUpdate' is not defined"
     ]
    }
   ],
   "source": [
    "#running LDA analysis to get parameters for movie training data\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "movieMatrix = make_word_matrix(RatingsDict, 1)\n",
    "movieOutput = LDA(6, movieMatrix, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the above parameters to make predictions for the test set\n",
    "#hold out one movie randomly, then determine the likelihood of the possible held out movies\n",
    "#the highest likelihood should be assigned to the last movie, if the model works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps\n",
    "\n",
    "- Compare output to Python package\n",
    "- Test on corpus in paper\n",
    "- Model topics on different corpus\n",
    "- Time and optimize (use Cython, quite slow now)\n",
    "- Run collaborative filtering\n",
    "- Compare to Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare our output to Python Package\n",
    "\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "for i in docs:\n",
    "    stemmed_tokens = p_stemmer.stem(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"The quick brown fox\"\n",
    "doc_b = \"Brown fox jumps over the jumps jumps jumps\"\n",
    "doc_c =  \"The the the lazy dog elephant.\"\n",
    "doc_d = \"The the the the the dog peacock lion tiger elephant\"\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, iterations = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta =  LDATest(3, corpusMatrix, 0.1, 1)[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostCommon(beta, corpusMatrix[1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpusMatrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k = number of topics, D = number of documents#\n",
    "#corpus matrix is output of make_word_matrix# \n",
    "def LDATest(k, corpusMatrix, tol, needToSplit):\n",
    "\n",
    "    \n",
    "    ##Check for proper input##\n",
    "    if isinstance(k, int) != True or k <= 0:\n",
    "        print(\"Number of topics must be a positive integer\")\n",
    "        return;\n",
    "    \n",
    "    if tol <=0:\n",
    "        print(\"Convergence tolerance must be positive\")\n",
    "        return;\n",
    "    \n",
    "    if needToSplit != 0 and needToSplit != 1:\n",
    "        print(\"NeedToSplit argument must be 0 or 1\")\n",
    "        return;\n",
    "    \n",
    "    \n",
    "    \n",
    "    M = corpusMatrix[2]\n",
    "    output = []\n",
    "    \n",
    "    converge = 0\n",
    "    iterations = 0\n",
    "    #initialize alpha and beta for first iteration\n",
    "    alphaOld = 10*np.random.rand(k)\n",
    "    V = corpusMatrix[0][0].shape[1]\n",
    "    betaOld = np.random.rand(k, V)\n",
    "    betaOld = betaOld/np.sum(betaOld, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "\n",
    "    phi = []\n",
    "    gamma = []\n",
    "    #looping through the number of documents\n",
    "    for d in range(0,M): #M is the number of documents\n",
    "        phiT, gammaT = Estep(k, d, alphaOld, betaOld, corpusMatrix, tol)\n",
    "        phi.append(phiT)\n",
    "        gamma.append(gammaT)\n",
    "\n",
    "    alphaNew, betaNew = Mstep(k, M, phi, gamma, alphaOld, corpusMatrix, tol)\n",
    "\n",
    "\n",
    "    converge =0\n",
    "    alphaOld = alphaNew\n",
    "    betaOld = betaNew\n",
    "    iterations += iterations\n",
    "    output.append([phi, gamma, alphaNew, betaNew])\n",
    "        \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
